<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="https://www.javiergamazo.com//blog/feed.xml" rel="self" type="application/atom+xml" /><link href="https://www.javiergamazo.com//blog/" rel="alternate" type="text/html" /><updated>2021-02-03T22:19:33+00:00</updated><id>https://www.javiergamazo.com//blog/feed.xml</id><title type="html">DidYouKnow</title><subtitle>The place where I talk about anything</subtitle><entry><title type="html">¿Qué vería a través un agujero de gusano?</title><link href="https://www.javiergamazo.com//blog/ray/tracing/2021/01/19/wormhole_es.html" rel="alternate" type="text/html" title="¿Qué vería a través un agujero de gusano?" /><published>2021-01-19T09:00:00+00:00</published><updated>2021-01-19T09:00:00+00:00</updated><id>https://www.javiergamazo.com//blog/ray/tracing/2021/01/19/wormhole_es</id><content type="html" xml:base="https://www.javiergamazo.com//blog/ray/tracing/2021/01/19/wormhole_es.html">&lt;p&gt;La ciencia ficción siempre ha fantaseado con la idea de viajar distancias imposibles utilizando instrumentos que alteren las propiedades del espacio a su aldedor. Este es el caso de la serie de videojuegos Portal, en la que el protagonista se vale del “Aperture Science Handheld Portal Device” para crear portales que conectan puntos del espacio de otra forma desconectados. Si bien en el juego se refieren a este tipo de estructuras como “portales”, su nombre real tanto en el mundo académico como en la cultura popular es el de “agujeros de gusano” desde que Charles Misner y John A. Wheeler acuñaran este término en 1957 &lt;a class=&quot;citation&quot; href=&quot;#Misner1957&quot;&gt;(Misner and Wheeler)&lt;/a&gt;. Aquí, se referían a los agujeros de gusano como puntos que conectarían regiones del espacio y del tiempo por medio de intensas deformaciones en el tejido espacio-temporal. Sin embargo, no fue hasta 2014 en la película Interstellar, cuando se le presentó al público una imagen de un agujero de gusano que pretendía ser exacta y acorde con las leyes físicas que conocemos hasta ahora &lt;a class=&quot;citation&quot; href=&quot;#James2015&quot;&gt;(James et al.)&lt;/a&gt;. Se debe aclarar, no obstante, que nunca se ha divisado un agujero de gusano debido tanto a las dificultades técnias que esto conllevaría &lt;a class=&quot;citation&quot; href=&quot;#Dai2019&quot;&gt;(Dai and Stojkovic)&lt;/a&gt; como a la incertidumbre que existe actualmente en torno a su (no) existencia. La existencia de agujeros de gusano requeriría tanto la posibilidad de aparición de energía negativa como de curvas cerradas de tipo tiempo (es decir, de viajes en el tiempo) &lt;a class=&quot;citation&quot; href=&quot;#Friedman2008&quot;&gt;(Friedman and Higuchi)&lt;/a&gt;. Además, de existir, muy probablemente este tipo de constructos se cerraría casi inmediatamente tras aparecer, por lo que la posibilidad de viajar a través de ellos sería limitada, más aún teniendo en cuenta que no sería un viaje cómodo debido a las intensas fuerzas de compresión y extensión a las que estaría sometido nuestro cuerpo al atravesar una región del espacio tiempo con una curvatura semejante. Aun con todo, ¿a quién no le produciría al menos una pizca de curiosidad el mirar a través de uno de ellos desde la comodidad de su sofá?&lt;/p&gt;

&lt;p&gt;Las formas de representar un agujero de gusano son muy variadas, empezando por &lt;a href=&quot;https://www.nasa.gov/centers/glenn/images/content/101681main_CD1998_76634_1200x900.jpg&quot; target=&quot;_blank&quot;&gt;las más artísticas&lt;/a&gt;, pasando por modelos confeccionados con la ayuda de programas de CGI (&lt;em&gt;Computer Generated Imagery&lt;/em&gt;) hasta las simulaciones más precisas como son las que podemos encontrar en la película. En este post trataré de explicar cómo podemos conseguir replicar el agujero de gusano que aparece en la película Interstellar, e intentaré hacerlo sin mostrar ecuaciones ni código (ya están los &lt;em&gt;papers&lt;/em&gt; y GitHub para eso) y partiendo de la base de la que partía yo cuando acometí este proyecto hace poco más de un mes: sin saber nada sobre el tema más allá de las ideas generales. Es importante recalcar la extrema complejidad de todo lo que voy a tratar aquí, por lo que simplificaré algunos detalles que no tendrán impacto en el conjunto, pero sí serán importantes a la hora de adquirir un conocimiento en profundidad de los temas que voy a tocar. En los siguientes apartados explicaré los fundamentos de la técnica de trazado de rayos (&lt;em&gt;Ray Tracing&lt;/em&gt;) que utilizaremos para la simulación, cómo aplicarla al espacio curvo y los pasos que hay que seguir hasta dar con la imagen final. En la Figura 1 se muestra el resultado que he obtenido.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/2.wormhole/images/result.png&quot; alt=&quot;Agujero de gusano&quot; width=&quot;850px&quot; class=&quot;image-centered&quot; /&gt;
&lt;em&gt;Figura 1: Imagen del agujero de gusano que se ha obtenido siguiendo el método que se describe en este post.&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;tracemos-unos-rayos&quot;&gt;Tracemos unos rayos&lt;/h3&gt;

&lt;p&gt;Cuando miramos un objeto, ¿qué es lo que vemos? Esa es la pregunta fundamental que debemos hacernos antes de empezar con este proyecto, puesto que de su respuesta dependerá el enfoque que le demos. Cuando miramos un objeto nuestro cerebro está registrando la intensidad de luz que portan los rayos que chocan contra los receptores de nuestra retina. Estos rayos proceden de las fuentes de luz que tenemos en nuestro entorno y adquieren una intensidad u otra en función de los objetos contra los que impactan. Así, cuando estamos al aire libre y vemos una manzana de color rojo es porque algunos rayos procedentes del sol han chocado contra ella y ésta nos ha devuelto la parte roja de los mismos. Una cámara se vale de un principio muy similar para capturar una fotografía: en el cuerpo de la cámara se encuentra el sensor, organizado en píxeles formando una cuadrícula, y cada uno de estos píxeles registra la intensidad de la luz que choca contra él. Este mismo principio se aplica para generar imágenes por ordenador, se colocan en la escena los elementos principales (cámara, luces y objetos) y en un proceso denominado &lt;em&gt;renderizado&lt;/em&gt; se traza la trayectoria de los rayos que impactan contra la cámara atendiendo además a sus rebotes y refracciones a los que éstos son sometidos. Esta técnica se denomina “Ray Tracing” y es fundamental en los procesos de renderizado modernos&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;El trazado de rayos comienza colocando la cámara en un punto de la escena y ¿recuerdas que una cámara tiene un sensor que forma una cuadrícula y es ahí donde se proyecta la imagen? Utilizaremos este mismo concepto. A una distancia de la cámara (la llamaremos distancia focal) colocaremos el plano de la pantalla, como en la Figura 2. Será en este plano donde se formará la imagen. Parece evidente que no podemos saber a priori qué rayos llegarán a cada píxel de la pantalla, y trazar todos los rayos posibles desde la fuente de luz parece una estrategia abocada al fracaso puesto que la mayoría ni siquiera rozarán la pantalla. No obstante, podemos valernos de que el comportamiento de los rayos es determinista y trazarlos al revés, desde la cámara hacia la escena, haciéndolos rebotar con los objetos y hasta que confluyan en una fuente de luz o se pierdan en el infinito mar de rebotes y refracciones. Así, el concepto de cámara pasa a ser tan solo el punto desde el que parten todos los rayos. Con cada choque el valor de la intensidad del rayo de luz cambiará en función de los parámetros del objeto (no es lo mismo una bola de cristal transparente que un cubo de madera de color marrón). Es importante señalar que en escenas cotidianas estos rayos se moverán en línea recta entre choques, de ninguna forma curvarán sus trayectorias (así es como se comporta la luz, generalmente viaja en línea recta), por lo que será inmediato saber para cada momento de tiempo dónde se encuentra un rayo de luz. Este será uno de los puntos a tener en cuenta al introducir el agujero de gusano.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/2.wormhole/images/screen_es.png&quot; alt=&quot;Situación de la cámara&quot; width=&quot;500px&quot; class=&quot;image-centered&quot; /&gt;
&lt;em&gt;Figura 2: Situación de la cámara y la pantalla.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;No merece la pena entrar en el detalle de cómo calcular la dirección de un rayo después de un rebote ni su intensidad, pero dejo aquí un libro que me ha sido de mucha ayuda en mi todavía corto viaje por el Ray Tracing: &lt;a href=&quot;https://raytracing.github.io/books/RayTracingInOneWeekend.html&quot;&gt;&lt;em&gt;Ray Tracing in One Weekend&lt;/em&gt;&lt;/a&gt;&lt;a class=&quot;citation&quot; href=&quot;#Shirley2020&quot;&gt;(Shirley)&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;diseñando-un-agujero-de-gusano&quot;&gt;Diseñando un agujero de gusano&lt;/h3&gt;

&lt;p&gt;Existen multitud de formas diferentes de diseñar un agujero de guano tal que sea físicamente plausible&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;. La primera vez que se consiguió fue en 1973, cuando H. G. Ellis &lt;a class=&quot;citation&quot; href=&quot;#Ellis1973&quot;&gt;(Ellis)&lt;/a&gt; y K. A. Bronnikov &lt;a class=&quot;citation&quot; href=&quot;#Bronnikov1973&quot;&gt;(Bronnikov)&lt;/a&gt; (por separado) desarrollaron un modelo matemático del agujero de gusano tal y como lo conocemos ahora, en el que en principio podría existir traspaso de materia entre los dos lados del mismo. Este agujero en realidad atraía intensanmente a la materia hacia su garganta (el centro del agujero) por uno de sus extremos, mientras que la repulsión del otro extremo era igualmente intensa. Por este motivo, se le denominó &lt;em&gt;Ellis drainhole&lt;/em&gt; (&lt;em&gt;drain&lt;/em&gt; significa desagüe en inglés y &lt;em&gt;hole&lt;/em&gt; es agujero).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/2.wormhole/images/ellis_wormhole.png&quot; alt=&quot;Agujero de gusano de Ellis&quot; width=&quot;350px&quot; class=&quot;image-centered&quot; /&gt;
&lt;em&gt;Figura 3: Agujero de gusano de Ellis.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Si bien en los años siguientes se desarrollaron otros agujeros de gusano con características variopintas (incluso algunos que permitirían viajar en el tiempo &lt;a class=&quot;citation&quot; href=&quot;#Morris1988&quot;&gt;(Morris et al.)&lt;/a&gt;), debido a su simplicidad matemática trabajaremos con una versión ligeramente modificada del agujero de gusano de Ellis para generar nuestra imagen. ¿Por qué una versión modificada? La razón es muy sencilla: al parecer Christopher Nolan (el director de la película) quería tener control sobre la forma del agujero -el radio de la garganta y la distancia entre los extremos principalmente- y eso es algo que el agujero de gusano de Ellis no ofrece, así que desarrollaron una modificación para la película &lt;a class=&quot;citation&quot; href=&quot;#James2015&quot;&gt;(James et al.)&lt;/a&gt;. El estudio de animación que se encargó de todo esto se llama &lt;a href=&quot;https://www.dneg.com/&quot;&gt;DNEG&lt;/a&gt; así que (oh, sorpresa) bautizaron a su agujero de gusano… Dneg.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/2.wormhole/images/dnegWormhole_es.png&quot; alt=&quot;Agujero de gusano Dneg&quot; width=&quot;350px&quot; class=&quot;image-centered&quot; /&gt;
&lt;em&gt;Figura 4: Agujero de gusano Dneg.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Como parámetros para nuestro agujero utilizaremos un radio de la garganta (\( \rho \)) de 1km, una altura de la garganta (\( a \)) de 0.01km y situaremos la cámara a 6.25km de la boca superior del agujero (en la posición de la Figura anterior), apuntando directamente a la boca del mismo. Como sabemos, se unirán dos partes diferentes del universo, cada una con su &lt;a href=&quot;https://es.wikipedia.org/wiki/Esfera_celeste&quot;&gt;esfera celeste&lt;/a&gt; (sí, así se llama a la esfera alrededor del observador en la que se colocan los astros), así que la cámara estará rodeada por un cielo estrellado y apuntará hacia el planeta Saturno, que se encontrará al otro lado del agujero&lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;. Las imágenes están disponibles en el &lt;a href=&quot;https://www.dneg.com/interstellar-wormhole/&quot;&gt;material suplementario del paper&lt;/a&gt;. Para imaginarlo -aunque no sea físicamente correcto-, podemos pensar en el agujero de la figura anterior con una esfera hueca a cada lado de la garganta en cuyo interior pegaremos una imagen con el contenido de esa porción del universo, esas serán las dos esferas celestes. Con esto ya tenemos todos los ingredientes necesarios para empezar a tomar nuestra foto.&lt;/p&gt;

&lt;h3 id=&quot;agárrate-que-vienen-curvas&quot;&gt;Agárrate, que vienen curvas&lt;/h3&gt;

&lt;p&gt;Ya hemos visto cómo tratar con los rayos que se mueven por el espacio plano al que estamos acostumbrados (espacio euclídeo). Sin embargo, la propia definición de agujero de gusano implica una deformación del espacio, curvándolo y afectando a todos los objetos que se encuentran a su alrededor, incluidos los rayos de luz. Por este motivo, la trayectoria de los rayos cambiará en función de cómo deforme el espacio el agujero de gusano que diseñemos. No parece necesario ahondar en las ecuaciones porque explicarlas llevaría demasiado tiempo, tan solo es necesario saber que para cada momento en el tiempo podremos obtener la posición exacta en la que se encuentra el rayo. Para el lector interesado, las ecuaciones se encuentran en el apéndice de &lt;a class=&quot;citation&quot; href=&quot;#James2015&quot;&gt;(James et al.)&lt;/a&gt;. Resolveremos las ecuaciones numéricamente y con ello obtendremos de qué lado del agujero proviene el rayo y en qué punto de la esfera celeste correspondiente. Como hemos visto en el apartado anterior, cada esfera celeste es en realidad una imagen, así que tan solo tendremos que tomar el valor de la imagen en el punto donde ha chocado el rayo, de esta forma tendremos un mapeo entre los píxeles de la cámara (punto inicial de los rayos) y los píxeles de las esferas celestes (punto final de los rayos). En la siguiente figura se muestra una representación &lt;em&gt;ficticia&lt;/em&gt; de este proceso para una de las dos esferas celestes.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/2.wormhole/images/celestialSphere_es.png&quot; alt=&quot;Esfera celeste&quot; width=&quot;650px&quot; class=&quot;image-centered&quot; /&gt;
&lt;em&gt;Figura 5: Mapeo de la esfera celeste a los píxeles de la pantalla. La representación es ficticia porque se está utilizando un espacio plano, entre la esfera celeste y la pantalla debería estar el agujero de gusano, por lo que las líneas no viajarían en línea recta.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Una vez obtenida la imagen podemos pasar a examinarla (Figura 1): vemos que la parte central se corresponde con la esfera celeste que hay en la parte del universo donde no está situada la cámara (Saturno), mientras que la parte externa es un cielo estrellado. Esto se debe precisamente a la curvatura del espacio. Como los rayos no viajan en línea recta, los píxeles del sensor de la cámara reciben luz desde ambos extremos, por lo que puede ocurrir (y de hecho ocurre) que dos píxeles contiguos reciban luz desde partes muy separadas del universo, de ahí que podamos ver los dos extremos de la boca del agujero a la vez. Vemos también en la imagen que el cielo estrellado alrededor del agujero ha dejado de ser “estrellado” para ser un amasijo de líneas curvas. Este efecto tiene su origen en la distorsion que el agujero crea en el espacio a su alrededor. Actúa como lente gravitacional, lo que implica hacer visible la luz que proviene de zonas que no deberían ser visibles si no hubiera agujero.&lt;/p&gt;

&lt;p&gt;Otro efecto muy interesante es el que se crea en el límite de la boca del agujero de gusano: en el extremo opuesto a donde está Saturno se puede ver una imagen secundaria de él (al menos una región mínima con píxeles marrones). La imagen primaria de Saturno aparece gracias a los rayos que viajan directamente desde el planeta hasta la cámara. Las secundarias (hay más pero no son visibles) provienen de rayos que han recorrido caminos más largos por el agujero de gusano. Este efecto aparece también en agujeros negros y es bien conocido &lt;a class=&quot;citation&quot; href=&quot;#Tsukamoto2012&quot;&gt;(Tsukamoto et al.)&lt;/a&gt;. Además, como depende de los rayos que han recorrido un camino más largo hasta llegar a la cámara, su presencia depende de la longitud del agujero. En la siguiente figura se puede observar este fenómeno en un agujero con una longitud 100 veces mayor, con lo que se ve claramente la segunda imagen de Saturno. Se ha modificado también la curvatura de la boca del agujero al obtener esa imagen, provocando que la deformación en el espacio circundante sea también mayor.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/2.wormhole/images/05w_1a.jpg&quot; alt=&quot;Agujero de gusano modificado&quot; width=&quot;850px&quot; class=&quot;image-centered&quot; /&gt;
&lt;em&gt;Figura 6: Imagen de un agujero de gusano Dneg. El aumento de la curvatura y la longitud de la garganta ocasionan una deformación todavía mayor del espacio alrededor.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Con esto concluyo esta entrada en la que he tratado de explicar cómo generar una imagen de un agujero de gusano utilizando tan solo las ecuaciones que lo gobiernan y una técnica muy conocida como lo es la de trazado de rayos. He querido transmitir en todo momento que este tema es extraordinariamente complejo y, por supuesto, hay detalles que he pasado por alto por miedo a que se volviera más denso si cabe.&lt;/p&gt;

&lt;p&gt;El código que he escrito para generar todas las imágenes está disponible públicamente en un repositorio de &lt;a href=&quot;https://github.com/javirk/Wormhole-simulation&quot;&gt;GitHub&lt;/a&gt;. No está tan ordenado como me gustaría, pero era la primera vez que utilizaba C++/C en muchos años. Cualquier duda o comentario es bien recibida.&lt;/p&gt;

&lt;hr /&gt;

&lt;hr /&gt;

&lt;ol class=&quot;bibliography&quot;&gt;&lt;li&gt;&lt;span id=&quot;Misner1957&quot;&gt;Misner, Charles W., and John A. Wheeler. “Classical Physics as Geometry.” &lt;i&gt;Annals of Physics&lt;/i&gt;, vol. 2, no. 6, Academic Press, December 1957, pp. 525–603, doi:10.1016/0003-4916(57)90049-0.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;James2015&quot;&gt;James, Oliver, et al. “Visualizing Interstellar’s Wormhole.” &lt;i&gt;Citation: American Journal of Physics&lt;/i&gt;, vol. 83, 2015, p. 486, doi:10.1119/1.4916949.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Dai2019&quot;&gt;Dai, De Chang, and Dejan Stojkovic. “Observing a Wormhole.” &lt;i&gt;Physical Review D&lt;/i&gt;, vol. 100, no. 8, American Physical Society, October 2019, p. 083513, doi:10.1103/PhysRevD.100.083513.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Friedman2008&quot;&gt;Friedman, John L., and Atsushi Higuchi. &lt;i&gt;Topological Censorship and Chronology Protection&lt;/i&gt;. 2008.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Shirley2020&quot;&gt;Shirley, Peter. &lt;i&gt;Ray Tracing in One Weekend&lt;/i&gt;. 2020, https://raytracing.github.io/books/RayTracingInOneWeekend.html.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Ellis1973&quot;&gt;Ellis, Homer G. “Ether Flow through a Drainhole: A Particle Model in General Relativity.” &lt;i&gt;Journal of Mathematical Physics&lt;/i&gt;, vol. 14, no. 1, American Institute of PhysicsAIP, January 1973, pp. 104–18, doi:10.1063/1.1666161.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Bronnikov1973&quot;&gt;Bronnikov, K. A. “Scalar-Tensor Theory and Scalar Charge.” &lt;i&gt;Acta Phys.Polon.B&lt;/i&gt;, vol. 4, 1973, pp. 251–66.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Morris1988&quot;&gt;Morris, Michael S., et al. “Wormholes, Time Machines, and the Weak Energy Condition.” &lt;i&gt;Physical Review Letters&lt;/i&gt;, vol. 61, no. 13, 1988, pp. 1446–49, doi:10.1103/PhysRevLett.61.1446.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Tsukamoto2012&quot;&gt;Tsukamoto, Naoki, et al. &lt;i&gt;Can We Distinguish between Black Holes and Wormholes by Their Einstein-Ring Systems?&lt;/i&gt; 2012.&lt;/span&gt;&lt;/li&gt;&lt;/ol&gt;
&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;La técnica de Ray Tracing no es la única actualmente pero sí una de las más utilizadas porque ofrece una buena relación entre la calidad final de las imágenes y el tiempo empleado para renderizar. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Que un agujero de gusano sea físicamente plausible implica que es una solución de las ecuaciones de Einstein, no que haya sido observado. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Se ha prescindido de colocar la cámara en la Figura 3 porque ésta es una representación del agujero en menos dimensiones de las que en realidad toman parte. Está bien para hacerse una idea de cara a la programación colocar la cámara mentalmente en la vertical de la garganta del agujero, apuntando perpendicularmente hacia ésta. Sin embargo, en dicha Figura en realidad se está representando el espacio-tiempo en sí, y todos los objetos deben estar sobre la retícula, incluida la cámara. &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name></name></author><category term="ray" /><category term="tracing" /><category term="wormhole" /><category term="&quot;ray" /><category term="tracing&quot;" /><summary type="html">La ciencia ficción siempre ha fantaseado con la idea de viajar distancias imposibles utilizando instrumentos que alteren las propiedades del espacio a su aldedor. Este es el caso de la serie de videojuegos Portal, en la que el protagonista se vale del “Aperture Science Handheld Portal Device” para crear portales que conectan puntos del espacio de otra forma desconectados. Si bien en el juego se refieren a este tipo de estructuras como “portales”, su nombre real tanto en el mundo académico como en la cultura popular es el de “agujeros de gusano” desde que Charles Misner y John A. Wheeler acuñaran este término en 1957 (Misner and Wheeler). Aquí, se referían a los agujeros de gusano como puntos que conectarían regiones del espacio y del tiempo por medio de intensas deformaciones en el tejido espacio-temporal. Sin embargo, no fue hasta 2014 en la película Interstellar, cuando se le presentó al público una imagen de un agujero de gusano que pretendía ser exacta y acorde con las leyes físicas que conocemos hasta ahora (James et al.). Se debe aclarar, no obstante, que nunca se ha divisado un agujero de gusano debido tanto a las dificultades técnias que esto conllevaría (Dai and Stojkovic) como a la incertidumbre que existe actualmente en torno a su (no) existencia. La existencia de agujeros de gusano requeriría tanto la posibilidad de aparición de energía negativa como de curvas cerradas de tipo tiempo (es decir, de viajes en el tiempo) (Friedman and Higuchi). Además, de existir, muy probablemente este tipo de constructos se cerraría casi inmediatamente tras aparecer, por lo que la posibilidad de viajar a través de ellos sería limitada, más aún teniendo en cuenta que no sería un viaje cómodo debido a las intensas fuerzas de compresión y extensión a las que estaría sometido nuestro cuerpo al atravesar una región del espacio tiempo con una curvatura semejante. Aun con todo, ¿a quién no le produciría al menos una pizca de curiosidad el mirar a través de uno de ellos desde la comodidad de su sofá?</summary></entry><entry><title type="html">Can an AI make up a language?</title><link href="https://www.javiergamazo.com//blog/ai/2020/12/25/discussing-networks_en.html" rel="alternate" type="text/html" title="Can an AI make up a language?" /><published>2020-12-25T09:00:00+00:00</published><updated>2020-12-25T09:00:00+00:00</updated><id>https://www.javiergamazo.com//blog/ai/2020/12/25/discussing-networks_en</id><content type="html" xml:base="https://www.javiergamazo.com//blog/ai/2020/12/25/discussing-networks_en.html">&lt;p&gt;The question in the header is recurring in all AI circles. Those who work on NLP (Natural Language Processing) strive to develop systems capable of processing language produced by human beings and take according actions. GPT-3 is an example of this, the goal of this model is just to predict (although very accurately) the most likely word following a text sequence &lt;a class=&quot;citation&quot; href=&quot;#Brown2020&quot;&gt;(Brown et al.)&lt;/a&gt;. However, due to its training process, one cannot defend that the model has a deep comprehension of the words it is producing nor that it has an objective in mind by expressing them. I believe the leap we as human beings have ahead to cover this last hole is still huge and it will not come in the following years. After all, we still do not understand how human mind works. So the answer to the question in the header will be “it depends”. It depends on what we are referring to when we say “language”.&lt;/p&gt;

&lt;p&gt;It is therefore relevant to know what we mean by “language” when we talk about it in this post. We will be referring to a common language shared by several systems through which they understand each other and transmit ideas. However, these ideas will come predefined by an agent external to the AI. A basic example could be to transmit a random number from 0 to 9 from system A to B in a common language, that B would process it, calculate its double, return it to A and A would receive the correct answer (the double of the first number). In this post we’ll try to lay the foundation for a project that meets all those requirements and one more condition: that the language shared by both systems is similar to that of the Star Wars R2-D2 Droid.&lt;/p&gt;

&lt;h3 id=&quot;it-takes-two-to-tango&quot;&gt;It takes two to tango&lt;/h3&gt;

&lt;p&gt;Indeed, for communication to exist there must be at least two systems: a transmitter of information and a receiver. Following the Communication Theory of Claude E. Shannon &lt;a class=&quot;citation&quot; href=&quot;#Shannon1948&quot;&gt;(Shannon)&lt;/a&gt;, it is also necessary a source of information, a channel, a message, a destination and a noise source. While the precise definition of these components is left to the reader’s discretion, it is of paramount importance for this project to distinguish between &lt;span style=&quot;color:blue&quot;&gt;information source&lt;/span&gt; and &lt;span style=&quot;color:red&quot;&gt;transmitter&lt;/span&gt;, and &lt;span style=&quot;color:blue&quot;&gt;destination&lt;/span&gt; and &lt;span style=&quot;color: red&quot;&gt;receiver&lt;/span&gt;: when we talk about transmitter we refer to the technical system in charge of encoding the information source to a set of signals suitable for the channel (and vice versa for the receiver, which decodes it so that the receiver can use the information). Applied to verbal communication between two people A and B, the source of information will be A’s idea, whose brain will be in charge of coding it in the form of words - message - that will be sent through the air - channel - until it reaches B. B’s brain will decode the words and generate an idea, which in the best case will be the same as the idea of A’s brain.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/1.discussing/images/information_theory_diagram_en.png&quot; alt=&quot;Information theory&quot; class=&quot;image-centered&quot; width=&quot;700px&quot; /&gt;
&lt;em&gt;Figure 1: Shannon’s Information Theory diagram. Adapted from &lt;a class=&quot;citation&quot; href=&quot;#Shannon1948&quot;&gt;(Shannon)&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Let us now transform this theory into the field of AI. As we know, neural networks are tremendously effective in generating representations from data, so we could face two of them (subjects A and B) and make them share messages. A would encode the message and B would… decode it? And what would happen if B wanted to transmit an idea to A? To find the solution to this question we can look at the most efficient system we know when it comes to producing and processing language: the brain. This organ takes care of both functions simultaneously (or can’t you listen to music and talk at the same time?), and that is because the information flow passes through separate areas&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;. These two zones are Wernicke’s area and Broca’s area, which are in charge of understanding and producing language respectively &lt;a class=&quot;citation&quot; href=&quot;#Neil2012&quot;&gt;(Neil)&lt;/a&gt;. Following this example, it seems logical to make our subjects be formed by two different and disconnected networks. One of these networks, the &lt;em&gt;encoder&lt;/em&gt; will be in charge of producing the language while the &lt;em&gt;decoder&lt;/em&gt; will try to understand it. In case A wants to transmit an idea to B, A’s &lt;em&gt;encoder&lt;/em&gt; will be responsible for transforming this idea into a common language and B’s &lt;em&gt;decoder&lt;/em&gt; will interpret it.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/1.discussing/images/estructura.png&quot; alt=&quot;Structure&quot; width=&quot;500px&quot; class=&quot;image-centered&quot; /&gt;
&lt;em&gt;Figure 2: Structure of a system with two subjects.&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;talking-as-clear-as-r2-d2&quot;&gt;Talking as clear as R2-D2&lt;/h3&gt;

&lt;p&gt;I will first describe R2-D2’s way of communicating for those who do not know it. It is a series of beeps, whistles and other sounds agglutinated to form something that resembles phrases &lt;a class=&quot;citation&quot; href=&quot;#Bray2015&quot;&gt;(Bray et al.)&lt;/a&gt;. On YouTube there are &lt;a href=&quot;https://www.youtube.com/watch?v=2-BKjnAgNgY&quot; target=&quot;_blank&quot;&gt;several&lt;/a&gt; videos showing these sounds. We will try to make the neural networks imitate this language when speaking, forcing the representation of the idea by the encoder to also be a series of beeps. However, forcing this type of representation is not trivial for two reasons: first, language must be created spontaneously through conversation between the networks, there must be no human interaction in this process. Secondly, we must define what we mean by “representation” in this context since language is very varied. We will address the second question first, which will lead us inexorably to the solution of the first._&lt;/p&gt;

&lt;p&gt;A sound can be represented in three different ways depending on what we want to know about it:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;The temporal representation&lt;/strong&gt; shows the intensity of the sound as a function of time.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Representation in frequencies:&lt;/strong&gt; explained very briefly, all sound, because it is a wave as a function of time, can be broken down into more basic waves with different frequencies. The method by which we go from a wave in the time domain to a wave in the frequency domain is called &lt;em&gt;Fourier transform&lt;/em&gt;. The representation in frequencies contains information on how important (amplitude) each frequency is in giving rise to the underlying sound.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Representation in the form of a spectrogram:&lt;/strong&gt; the two previous representations have two dimensions, i.e. amplitude as a function of time or amplitude of each frequency. However, we can join both to produce a three-dimensional representation: time, amplitude and frequency. A spectrogram shows the evolution of frequency and intensity in time. Typically, intensity is defined by color, while time takes the abscissa axis and frequency the ordinate axis. Unlike the previous cases, a spectrogram can be saved as an image since the relevant information is coded both in the color of the pixels and in their position. The following figure shows the spectrogram of a voice fragment of R2-D2&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/1.discussing/images/espectrograma.png&quot; alt=&quot;Spectrogram&quot; class=&quot;image-centered&quot; /&gt;
&lt;em&gt;Figure 3: Spectrogram of a sound produced by R2-D2. It has been calculated by taking the Fourier transform of all the audio of the video in &lt;a href=&quot;https://www.youtube.com/watch?v=2-BKjnAgNgY&quot;&gt;https://www.youtube.com/watch?v=2-BKjnAgNgY&lt;/a&gt; and dividing the result in 256 pixels images.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;It has been shown that neural networks operate well with tabulated data and especially well with images &lt;a class=&quot;citation&quot; href=&quot;#Krizhevsky2012&quot;&gt;(Krizhevsky et al.)&lt;/a&gt;&lt;a class=&quot;citation&quot; href=&quot;#He2016&quot;&gt;(He et al.)&lt;/a&gt;, so it seems logical to use the spectrogram for this task. In general, convolutional neural networks are used to work with images because they are capable of capture relations in their pixels, usually by identifying borders in different directions or other coarser features like textures. Explaining in detail how this type of networks work would be useful for another article, so it is left to the interested reader to become familiar with this type of architectures, since there are many resources available (&lt;a class=&quot;citation&quot; href=&quot;#Cs231n&quot;&gt;(Stanford)&lt;/a&gt;, for example). I will only say here that with a network of convolutional layers we can obtain a multi-dimensional representation of the input data (an image). That is, we can get the translation of any number in an image, and this translation can be learned for a concrete task.&lt;/p&gt;

&lt;h3 id=&quot;a-spoonful-of-architecture&quot;&gt;A spoonful of architecture&lt;/h3&gt;

&lt;p&gt;Once all the technical aspects of the problem have been addressed, we will focus on defining the system architecture and the training process. Determining network architecture is usually more of an art than a science, so I will not go into detail. First, we will code the idea (the number) in a One-Hot way, which means that to transmit a digit from \( N \) possible ones we will use a vector of  \( N \) components with zero in all of them and one in the relevant component. For example, the vector for the number 1 in the interval \( [0, 4] \) will be \([0, 1, 0, 0, 0]\). To use this data with convolutional networks we will have to transform it to three dimensions ( (C\times H\times W ) \), increasing first the amount of components with at least one fully connected layer and resizing the result. Later, we will add convolutional layers so that the result is a map of the dimensions of the spectrogram we are looking for. As the task we are trying to solve is symmetric (the encoder and the decoder must perform opposite tasks), we will opt for symmetric architectures for these two networks. If we have said that the encoder will start with a fully connected layer followed by convolutional layers, the decoder will start with convolutional layers and finally another fully connected one. In this way, the diagram above takes on the following structure:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/1.discussing/images/estructura_espectrograma_en.png&quot; alt=&quot;Structure with spectrogram&quot; width=&quot;650px&quot; class=&quot;image-centered&quot; /&gt;
&lt;em&gt;Figure 3: Structure of a system with two subjects where number 1 is sent. By comparison with Shannon’s information theory, the central spectrogram shows the message. Only one part of the system is shown for the sake of simplicity.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;To make a neural network system behave in a certain way, one must specify a quantity to minimize. This amount is called &lt;em&gt;loss&lt;/em&gt;, and in many cases it is a function that depends on the network output and the original data&lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;. Applied to our problem, we can immediately define a quantity that the system should minimize: we want B to interpret the same idea that A is trying to transmit. As there is only one correct number each time, the problem is multiclass (predict a number and only one of ten possible) and we will minimize the &lt;em&gt;Cross-Entropy loss&lt;/em&gt;, defined as:
\[ L_{CE} = - \sum_i^C t_i \log(s_i),\]
where \( C \) are the possible classes and \( t_i \) and \( s_i \) represent the real and predicted labels, respectively&lt;a class=&quot;citation&quot; href=&quot;#Gomez2018&quot;&gt;(Gómez)&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;By minimizing the Cross-Entropy loss we get the two networks to agree on the number they are transmitting. It is important to point out that only by minimizing this function the networks would create an internal language at the output of the encoder, but this would very likely be random. As we want them to use R2-D2’s voice, we will have to add a function of loss more and we will do it… with style.&lt;/p&gt;

&lt;h3 id=&quot;speaking-with-style&quot;&gt;Speaking with style&lt;/h3&gt;

&lt;p&gt;As explained above, language must be created spontaneously and without human interaction, and for this purpose representation in the form of a spectrogram can help us. As it is evident, we cannot force the output of the encider to be equal (pixel by pixel) to a concrete spectrogram or to a group of these since we would be violating the second assumption of the problem (there would exist in that case strong human interaction). We will need then a less intrusive loss function, that does not look for differences by pixel but something more general. Fortunately, in 2015 a type of loss was introduced that is suitable for this use case: the style loss &lt;a class=&quot;citation&quot; href=&quot;#Gatys2015&quot;&gt;(Gatys et al.)&lt;/a&gt;. Intuitively, this loss is calculated by passing the images through a pre-trained neural network (VGG) and comparing the intermediate representations of the target image and the generated image. The authors apply this idea to transfer the style of a picture to a photo, keeping the underlying idea of the original photo intact. For anyone interested, mathematically it looks like this with \( l \) intermediate representations:&lt;/p&gt;

&lt;p&gt;\[ L_{style} = \sum_l w^lL_{style}^l,   \]
\[ L_{style}^l = \dfrac{1}{M^l} \sum_{ij} (G_{ij}^l(s) - G_{ij}^l(g))^2,   \]
where \( G(s) \) and \( G(g) \) refer to the Gram matrix of the style image and the generated image. For further information, I think &lt;a href=&quot;https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-neural-style-transfer-ef88e46697ee&quot;&gt;this article&lt;/a&gt; can be interesting.&lt;/p&gt;

&lt;p&gt;We can use the style loss to induce the output of the encoder to be similar to the reference spectrograms. This will presumably keep the sounds random but the result will remind us of the movie droid. However, this function cannot be applied to both encoder and decoder. Since only the encoder is involved in the language creation process, it is the only one that should receive feedback in this regard. However, the idea must flow through both networks, so the Cross-Entropy loss must spread throughout the system. Thus, the functions in each case result:&lt;/p&gt;

&lt;p&gt;\[ L_{decoder} = L_{CE} = - \sum_i^C t_i \log(s_i),\]
\[ L_{encoder} = L_{CE} + \lambda L_{style} = - \sum_i^C t_i \log(s_i) + \lambda \sum_l w^l \dfrac{1}{M^l} \sum_{ij} (G_{ij}^l(s) - G_{ij}^l(g))^2\]&lt;/p&gt;

&lt;p&gt;And the whole system is:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/1.discussing/images/estructura_loss_en.png&quot; alt=&quot;Structure with loss&quot; width=&quot;650px&quot; class=&quot;image-centered&quot; /&gt;
&lt;em&gt;Figure 4: Propagation and computation of the loss function throughout the system. Black: forward propagation. Red: backpropagation of the loss function.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;One has to notice that the simplicity of the message we are transmitting can be a problem in order to train the encoder to generate an adequate representation, as it may cause the loss function to be unbalanced. Later on, we will make transmission harder with the addition of noise to the intermediate representation, thus making the input of the decoder different from the output of the encoder, perturbing the system and avoiding \(L_{decoder} = L_{CE} = 0 \) after a couple of training steps.&lt;/p&gt;

&lt;p&gt;All that remains is choosing a training strategy. As described above, each subject is formed by two networks with different tasks and each communication faces different parts of each subject. Therefore, with \( N \) subjects we will have  \( 2N \) networks and  \( N^2 \) ways to face them (counting on the fact that we want a subject to be able to understand himself). To train all the subjects at the same time and avoid that some nets acquire more level than others we will have to follow a staggered strategy, alternating the training steps between all the combinations. Thus, both the time and the complexity of the training grow with \( \mathcal{O}(N^2) \). This strategy will also prevent the system from learning \( N^2 \) different languages, as all the networks will be learning in parallel.&lt;/p&gt;

&lt;p&gt;Thus concludes the first part of this article on how to devise a system of neural networks from which a common language can emerge and how to make this language have the form we want. The second part will explore the results and other training strategies based on these. In addition, a noise source will be included to make the training more robust.&lt;/p&gt;

&lt;hr /&gt;

&lt;hr /&gt;

&lt;ol class=&quot;bibliography&quot;&gt;&lt;li&gt;&lt;span id=&quot;Brown2020&quot;&gt;Brown, Tom B., et al. &lt;i&gt;Language Models Are Few-Shot Learners&lt;/i&gt;. May 2020, https://arxiv.org/abs/2005.14165.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Shannon1948&quot;&gt;Shannon, C. E. “A Mathematical Theory of Communication.” &lt;i&gt;Bell System Technical Journal&lt;/i&gt;, 1948, doi:10.1002/j.1538-7305.1948.tb01338.x.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Neil2012&quot;&gt;Neil, Carlson. “Physiology of Behavior.” &lt;i&gt;IEEE Transactions on Information Theory&lt;/i&gt;, 2012.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Bray2015&quot;&gt;Bray, Adam, et al. &lt;i&gt;Star Wars: Absolutely Everything You Need to Know&lt;/i&gt;. DK Children, 2015, p. 200.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Krizhevsky2012&quot;&gt;Krizhevsky, Alex, et al. “ImageNet Classification with Deep Convolutional Neural Networks.” &lt;i&gt;Advances in Neural Information Processing Systems&lt;/i&gt;, 2012, doi:10.1061/(ASCE)GT.1943-5606.0001284.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;He2016&quot;&gt;He, Kaiming, et al. “Deep Residual Learning for Image Recognition.” &lt;i&gt;Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition&lt;/i&gt;, 2016, doi:10.1109/CVPR.2016.90.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Cs231n&quot;&gt;Stanford, Cs231n. &lt;i&gt;CS231n Convolutional Neural Networks for Visual Recognition&lt;/i&gt;. https://cs231n.github.io/convolutional-networks/. Accessed December 21, 2020.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Gomez2018&quot;&gt;Gómez, Raúl. &lt;i&gt;Understanding Categorical Cross-Entropy Loss, Binary Cross-Entropy Loss, Softmax Loss, Logistic Loss, Focal Loss and All Those Confusing Names&lt;/i&gt;. May 2018, https://gombru.github.io/2018/05/23/cross_entropy_loss/.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Gatys2015&quot;&gt;Gatys, Leon, et al. “A Neural Algorithm of Artistic Style.” &lt;i&gt;Journal of Vision&lt;/i&gt;, 2015, doi:10.1167/16.12.326.&lt;/span&gt;&lt;/li&gt;&lt;/ol&gt;
&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Actually the Wernicke area and the Broca area are connected by the arcuate fasciculus &lt;a class=&quot;citation&quot; href=&quot;#Neil2012&quot;&gt;(Neil)&lt;/a&gt;. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Only the magnitude is shown in this spectrogram. To calculate it, the Fourier transform of the signal has to be computed, and that carries a phase that is not mentioned here but that is fundamental to make the inverse transform later. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;This applies for supervised models, where we know the value the model should predict for each input value. There are other types of algorithms (unsupervised, for example), in which this is not fulfilled and the loss function takes other forms. &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name></name></author><category term="AI" /><category term="AI" /><category term="image" /><category term="language" /><summary type="html">The question in the header is recurring in all AI circles. Those who work on NLP (Natural Language Processing) strive to develop systems capable of processing language produced by human beings and take according actions. GPT-3 is an example of this, the goal of this model is just to predict (although very accurately) the most likely word following a text sequence (Brown et al.). However, due to its training process, one cannot defend that the model has a deep comprehension of the words it is producing nor that it has an objective in mind by expressing them. I believe the leap we as human beings have ahead to cover this last hole is still huge and it will not come in the following years. After all, we still do not understand how human mind works. So the answer to the question in the header will be “it depends”. It depends on what we are referring to when we say “language”.</summary></entry><entry><title type="html">¿Puede una IA inventar un idioma?</title><link href="https://www.javiergamazo.com//blog/ia/2020/12/25/discussing-networks_es.html" rel="alternate" type="text/html" title="¿Puede una IA inventar un idioma?" /><published>2020-12-25T09:00:00+00:00</published><updated>2020-12-25T09:00:00+00:00</updated><id>https://www.javiergamazo.com//blog/ia/2020/12/25/discussing-networks_es</id><content type="html" xml:base="https://www.javiergamazo.com//blog/ia/2020/12/25/discussing-networks_es.html">&lt;p&gt;La pregunta que encabeza este post es recurrente en todos los círculos de la Inteligencia Artificial. Aquellos que trabajan en NLP (procesamiento de lenguaje natural por sus siglas en inglés) buscan concebir sistemas que sean capaces de entender -procesar- lenguaje producido por seres humanos y tomar acciones acordes. GPT-3 es un ejemplo de esto, el cometido de este modelo tan solo es predecir (aunque muy acertadamente) la palabra más probable que seguirá una secuencia de texto &lt;a class=&quot;citation&quot; href=&quot;#Brown2020&quot;&gt;(Brown et al.)&lt;/a&gt;. Sin embargo, por cómo ha sido entrenado, en ningún caso se puede defender que el modelo posea una comprensión profunda de las palabras que está produciendo ni que tenga un objetivo en particular al expresarlas. Si se me permite mi opinión, creo que el salto que nos queda por delante hasta cubrir este último punto es todavía enorme y no vendrá en los próximos años. Al fin y al cabo, todavía no entendemos cómo funciona la mente humana. Así que la respuesta a la pregunta del encabezado será… “depende”, depende de a lo que nos refiramos con la palabra idioma.&lt;/p&gt;

&lt;p&gt;Es por tanto relevante saber a qué nos referimos con “idioma” o “lenguaje” cuando hablemos de ello en este post. Nos estaremos refiriendo a un lenguaje común que compartan varios sistemas a través del cual puedan llegar a entenderse y transmitir ideas. No obstante, estas ideas vendrán predefinidas por un agente externo a la IA. Un ejemplo básico podría ser transmitir un número aleatorio de 0 a 9 del sistema A al B en un lenguaje común, que el B lo procesara, calculara su doble, lo devolviera a A y A interpretara el resultado correcto. En este post intentaremos sentar las bases de un proyecto que cumpla todos esos requisitos y una condición más: que el idioma que compartan ambos sistemas sea similar al del droide R2-D2 de Star Wars.&lt;/p&gt;

&lt;h3 id=&quot;dos-no-hablan-si-uno-no-quiere&quot;&gt;Dos no hablan si uno no quiere&lt;/h3&gt;

&lt;p&gt;Efectivamente, para que exista comunicación debe haber al menos dos sistemas: un emisor de información (transmisor) y otro receptor de la misma. Siguiendo la Teoría de la Comunicación de Claude E. Shannon &lt;a class=&quot;citation&quot; href=&quot;#Shannon1948&quot;&gt;(Shannon)&lt;/a&gt;, es necesario además una fuente de información, un canal, un mensaje, un destinatario y un elemento de ruido. Si bien la definición precisa de estos componentes se deja a discreción del lector, es de capital importancia para este proyecto distinguir entre &lt;span style=&quot;color:blue&quot;&gt;fuente de información&lt;/span&gt; y &lt;span style=&quot;color:red&quot;&gt;transmisor&lt;/span&gt;, y &lt;span style=&quot;color:blue&quot;&gt;destinatario&lt;/span&gt; y &lt;span style=&quot;color:red&quot;&gt;receptor&lt;/span&gt;: cuando hablamos de transmisor nos referimos al sistema técnico encargado de &lt;em&gt;codificar&lt;/em&gt; la fuente de información a un conjunto de señales aptas para el canal (y viceversa para el receptor, que la &lt;em&gt;decodifica&lt;/em&gt; para que el destinatario pueda utilizar la información). Aplicado a la comunicación verbal entre dos personas A y B, la fuente de información será la idea de A, cuyo cerebro se encargará de codificar en forma de palabras -mensaje- que se enviarán por el aire -canal- hasta llegar a B. El cerebro de B decodificará las palabras y generará una idea, que en el mejor de los casos será igual a la idea del cerebro de A.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/1.discussing/images/information_theory_diagram_es.png&quot; alt=&quot;Information theory&quot; class=&quot;image-centered&quot; width=&quot;700px&quot; /&gt;
&lt;em&gt;Figura 1: Diagrama de la teoría de la información de Shannon. Adaptado de &lt;a class=&quot;citation&quot; href=&quot;#Shannon1948&quot;&gt;(Shannon)&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Transformemos ahora esta teoría al campo de la IA. Como sabemos, las redes neuronales son tremendamente eficaces a la hora de generar representaciones a partir de datos, por lo que podríamos enfrentar dos de ellas (sujetos A y B) y hacerlas compartir mensajes. A codificaría el mensaje y B lo… ¿decodificaría? ¿Y qué pasaría si B quisiera transmitir una idea a A? Para encontrar la solución a esta pregunta podemos fijarnos en el sistema más eficiente que conocemos a la hora de producir y procesar lenguaje: el cerebro. Este órgano se encarga de ambas funciones simultáneamente (¿o es que no puedes escuchar música y hablar a la vez?), y eso se debe a que el flujo de información transcurre por zonas separadas&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;. Estas dos zonas son el área de Wernicke y el área de Broca, que se encargan de la comprensión y de la producción del lenguaje respectivamente &lt;a class=&quot;citation&quot; href=&quot;#Neil2012&quot;&gt;(Neil)&lt;/a&gt;. Siguiendo este ejemplo, parece lógico hacer que nuestros sujetos estén formados por dos redes diferentes y desconectadas. Una de estas redes, el &lt;em&gt;encoder&lt;/em&gt; se encargará de producir el lenguaje mientras que el &lt;em&gt;decoder&lt;/em&gt; tratará de comprenderlo. En caso de que A quiera transmitir una idea a B, será el &lt;em&gt;encoder&lt;/em&gt; de A el responsable de transformar esta idea a un lenguaje común y el &lt;em&gt;decoder&lt;/em&gt; de B lo interpretará.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/1.discussing/images/estructura.png&quot; alt=&quot;Estructura&quot; width=&quot;500px&quot; class=&quot;image-centered&quot; /&gt;
&lt;em&gt;Figura 2: Estructura de un sistema con dos individuos.&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;hablemos-tan-claro-como-r2-d2&quot;&gt;Hablemos tan claro como R2-D2&lt;/h3&gt;

&lt;p&gt;Describiré en primer lugar la forma de comunicarse de R2-D2 para aquellos que la desconozcan. Se trata de una serie de pitidos, silbidos y otros sonidos aglutinados para formar algo que se asemeja a frases &lt;a class=&quot;citation&quot; href=&quot;#Bray2015&quot;&gt;(Bray et al.)&lt;/a&gt;. En YouTube hay &lt;a href=&quot;https://www.youtube.com/watch?v=2-BKjnAgNgY&quot; target=&quot;_blank&quot;&gt;infinidad&lt;/a&gt; de vídeos mostrando estos sonidos. Intentaremos hacer que las redes neuronales imiten este lenguaje a la hora de hablar, forzando que la representación de la idea por parte del &lt;em&gt;encoder&lt;/em&gt; sea también una serie de pitidos. Sin embargo, forzar este tipo de representación no es trivial por dos motivos: en primer lugar, el lenguaje debe ser creado espontáneamente a través de la conversación entre las redes, no debe existir ningún tipo de interacción humana en este proceso. En segundo lugar, debemos definir a qué nos referimos con “representación” en este contexto puesto que el lenguaje es muy variado. Abordaremos primero la segunda cuestión, lo que nos llevará inexorablemente hasta la solución de la primera.&lt;/p&gt;

&lt;p&gt;Un sonido puede ser representado de tres formas diferentes en función de lo que busquemos conocer acerca del mismo:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Representación temporal:&lt;/strong&gt; muestra la intensidad del sonido en función del tiempo.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Representación en frecuencias:&lt;/strong&gt; explicado muy brevemente, todo sonido, por el hecho de ser una onda en función del tiempo, puede ser descompuesto en ondas más básicas con diferentes frecuencias. El método con el que pasamos de una onda en el dominio temporal a una onda en el dominio de frecuencias recibe el nombre de &lt;em&gt;transformada de Fourier&lt;/em&gt;. La representación en frecuencias contiene información de lo importante (amplitud) que es cada frecuencia para dar lugar al sonido subyacente.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Representación en forma de espectrograma:&lt;/strong&gt; las dos representaciones anteriores tienen dos dimensiones, i.e. amplitud en función del tiempo o amplitud de cada frecuencia. Sin embargo, podemos unir ambas para dar lugar a una representación tridimensional: tiempo, amplitud y frecuencia. Un espectrograma muestra la evolución de la frecuencia y de la intensidad en el tiempo. Típicamente, la intensidad se define por el color, mientras que el tiempo toma el eje de abscisas y la frecuencia, el de ordenadas. A diferencia de en los casos anteriores, un espectrograma puede ser guardado en forma de imagen ya que la informaciónm relevante está codificada tanto en el color de los píxeles como en su posición. La siguiente figura muestra el espectrograma de un fragmento de la voz de R2-D2&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/1.discussing/images/espectrograma.png&quot; alt=&quot;Espectrograma&quot; class=&quot;image-centered&quot; /&gt;
&lt;em&gt;Figura 3: Espectrograma de un sonido producido por R2-D2. Se ha calculado tomando la transformada de Fourier de todo el audio del vídeo en &lt;a href=&quot;https://www.youtube.com/watch?v=2-BKjnAgNgY&quot;&gt;https://www.youtube.com/watch?v=2-BKjnAgNgY&lt;/a&gt; y dividiendo el resultado en imágenes de 256 píxeles.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Se ha demostrado que las redes neuronales operan bien con datos tabulados y especialmente bien con imágenes &lt;a class=&quot;citation&quot; href=&quot;#Krizhevsky2012&quot;&gt;(Krizhevsky et al.)&lt;/a&gt;&lt;a class=&quot;citation&quot; href=&quot;#He2016&quot;&gt;(He et al.)&lt;/a&gt;, así que parece lógico utilizar el espectograma para esta tarea. En general, para trabajar con imágenes se utilizan redes neuronales convolucionales porque son capaces de capturar relaciones en ellas, normalmente a través de la identificación de bordes en diferentes direcciones y otras características más vagas como las texturas. Explicar en detalle cómo funcionan este tipo de redes daría para otro artículo, por lo que se deja al lector interesado el familiarizarse con este tipo de arquitecturas, ya que existen multitud de recursos disponibles (&lt;a class=&quot;citation&quot; href=&quot;#DotCSV2020&quot;&gt;(Dot CSV)&lt;/a&gt;, &lt;a class=&quot;citation&quot; href=&quot;#Cs231n&quot;&gt;(Stanford)&lt;/a&gt; [en inglés], por ejemplo). Tan solo diré aquí que con una red de capas convolucionales podemos obtener una representación en varias dimensiones de los datos de entrada (una imagen). Es decir, podemos conseguir la traducción de un número cualquiera en una imagen, y esta traducción puede ser aprendida para una tarea concreta.&lt;/p&gt;

&lt;h3 id=&quot;un-poco-de-arquitectura&quot;&gt;Un poco de arquitectura&lt;/h3&gt;

&lt;p&gt;Una vez abordados todos los aspectos técnicos del problema nos centraremos en definir la arquitectura del sistema y la forma de entrenamiento. Determinar la arquitectura de las redes generalmente tiene más de arte que de ciencia, por lo que no me explayaré en los detalles. En primer lugar, codificaremos la idea (el número) de forma &lt;em&gt;One-Hot&lt;/em&gt;, lo que significa que para transmitir un dígito de entre \( N \) posibles utilizaremos un vector de \( N \) componentes con cero en todas ellas y un uno en la componente relevante. Por ejemplo, el vector para el número 1 en el intervalo \( [0, 4] \) será \([0, 1, 0, 0, 0]\). Para utilizar estos datos con redes convolucionales deberemos transformarlos a tres dimensiones \( (C\times H\times W ) \), aumentando en primer lugar la cantidad de componentes con al menos una capa &lt;em&gt;fully connected&lt;/em&gt; y redimensionando el resultado. Posteriormente, añadiremos capas convolucionales de forma que el resultado sea un mapa de las dimensiones del espectrograma que buscamos. Como la tarea que buscamos resolver es simétrica (el &lt;em&gt;encoder&lt;/em&gt; y el &lt;em&gt;decoder&lt;/em&gt; deben realizar tareas opuestas), optaremos por arquitecturas simétricas para estas dos redes. Si hemos dicho que el &lt;em&gt;encoder&lt;/em&gt; empezará con una capa &lt;em&gt;fully connected&lt;/em&gt; seguida de capas convolucionales, el &lt;em&gt;decoder&lt;/em&gt; empezará por capas convolucionales y por último otra &lt;em&gt;fully connected&lt;/em&gt;. De esta forma, el diagrama anterior adquiere la siguiente estructura:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/1.discussing/images/estructura_espectrograma_es.png&quot; alt=&quot;Estructura con espectrograma&quot; width=&quot;650px&quot; class=&quot;image-centered&quot; /&gt;
&lt;em&gt;Figura 3: Estructura de un sistema con dos individuos en el que se envía el número 1. Por comparación con la teoría de la información de Shannon, el espectrograma central muestra el mensaje. Solo se muestra una parte del sistema (conexión de A con B) por simplicidad.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Para hacer que un sistema de redes neuronales se comporte de una forma determinada se debe especificar una cantidad a minimizar. A esta cantidad se le llama &lt;em&gt;loss&lt;/em&gt;, y en muchos casos es una función que depende de la salida de la red y de los datos originales&lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;. Aplicado a nuestro problema, podemos definir inmediatamente una cantidad que el sistema debe minimizar: queremos que B interprete la misma idea que está intentando transmitir A. Como solo hay un número correcto cada vez, el problema es multiclase (predecir un número y solo uno de entre diez posibles) y minimizaremos la &lt;em&gt;Cross-Entropy loss&lt;/em&gt;, definida como:
\[ L_{CE} = - \sum_i^C t_i \log(s_i),\]
donde \( C \) son las clases posibles y \( t_i \) y \( s_i \) representan la etiqueta real y la predicha por la red respectivamente &lt;a class=&quot;citation&quot; href=&quot;#Gomez2018&quot;&gt;(Gómez)&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Minimizando la &lt;em&gt;Cross-Entropy loss&lt;/em&gt; conseguimos que las dos redes se pongan de acuerdo en el número que están transmitiendo. Es importante puntualizar que tan solo minimizando esta cantidad las redes ya crearían un lenguaje interno a la salida del &lt;em&gt;encoder&lt;/em&gt;, pero éste sería probablemente aleatorio. Como queremos que utilicen la voz de R2-D2, tendremos que añadir una función de &lt;em&gt;loss&lt;/em&gt; más y lo haremos… con estilo.&lt;/p&gt;

&lt;h3 id=&quot;hablando-con-estilo&quot;&gt;Hablando con estilo&lt;/h3&gt;

&lt;p&gt;Como se ha explicado más arriba, el lenguaje debe ser creado espontáneamente y sin interacción humana, y para ello la representación en forma de espectrograma nos puede ayudar. Como es evidente, no podemos forzar que la salida del &lt;em&gt;encoder&lt;/em&gt; sea igual (píxel a píxel) a un espectrograma concreto ni a un grupo de estos puesto que estaríamos violando el segundo supuesto del problema (existiría en ese caso fuerte interacción humana). Necesitaremos entonces una función de &lt;em&gt;loss&lt;/em&gt; menos intrusiva, que no busque diferencias por píxel sino algo más general. Por suerte, en 2015 se introdujo un tipo de &lt;em&gt;loss&lt;/em&gt; idóneo para este caso de uso: la &lt;em&gt;loss&lt;/em&gt; de estilo &lt;a class=&quot;citation&quot; href=&quot;#Gatys2015&quot;&gt;(Gatys et al.)&lt;/a&gt;. Intuitivamente, esta &lt;em&gt;loss&lt;/em&gt; se calcula haciendo pasar las imágenes por una red neuronal preentrenada (VGG) y comparando las representaciones intermedias de la imagen objetivo y la imagen generada. Los autores aplican esta idea para transferir el estilo de un cuadro a una foto, manteniendo intacta la idea subyacente de la foto original. Para quien esté interesado, matemáticamente tiene este aspecto con \( l \) representaciones intermedias:&lt;/p&gt;

&lt;p&gt;\[ L_{estilo} = \sum_l w^lL_{estilo}^l,   \]
\[ L_{estilo}^l = \dfrac{1}{M^l} \sum_{ij} (G_{ij}^l(s) - G_{ij}^l(g))^2,   \]
donde \( G(s) \) y \( G(g) \) se refieren a la matriz de Gram de la imagen estilo y la imagen generada respectivamente. En esta &lt;em&gt;loss&lt;/em&gt; se está calculando la correlación entre las características extraídas en cada capa de la red preentrenada. Para más información, creo que &lt;a href=&quot;https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-neural-style-transfer-ef88e46697ee&quot;&gt;este artículo&lt;/a&gt; (en inglés) puede ser interesante.&lt;/p&gt;

&lt;p&gt;Podemos utilizar la &lt;em&gt;loss&lt;/em&gt; de estilo para inducir que la salida del &lt;em&gt;encoder&lt;/em&gt; sea similar a los espectrogramas de referencia. Con ello presumiblemente conseguiremos que los sonidos se mantengan aleatorios pero nos recuerden a los del droide de las películas. No obstante, esta función no puede ser aplicada a &lt;em&gt;encoder&lt;/em&gt; y &lt;em&gt;decoder&lt;/em&gt; por igual. Puesto que solo el &lt;em&gt;encoder&lt;/em&gt; participa en el proceso de creación del lenguaje, es el único que debe recibir &lt;em&gt;feedback&lt;/em&gt; en ese aspecto. Sin embargo, la idea debe fluir a través de ambas redes, por lo que la &lt;em&gt;Cross-Entropy loss&lt;/em&gt; se debe propagar por todo el sistema. Así, las funciones en cada caso resultan:
\[ L_{decoder} = L_{CE} = - \sum_i^C t_i \log(s_i),\]
\[ L_{encoder} = L_{CE} + \lambda L_{estilo} = - \sum_i^C t_i \log(s_i) + \lambda \sum_l w^l \dfrac{1}{M^l} \sum_{ij} (G_{ij}^l(s) - G_{ij}^l(g))^2\]&lt;/p&gt;

&lt;p&gt;Y el sistema total queda como en la Figura 4:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/1.discussing/images/estructura_loss_es.png&quot; alt=&quot;Estructura con loss&quot; width=&quot;650px&quot; class=&quot;image-centered&quot; /&gt;
&lt;em&gt;Figura 4: Propagación y cálculo de las funciones de _loss_ a través del sistema. En negro, propagación hacia delante. En rojo, propagación de las funciones de loss hacia atrás.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Hay que tener en cuenta que la simplicidad del mensaje que estamos transmitiendo puede suponer un problema a la hora de entrenar el &lt;em&gt;encoder&lt;/em&gt; para que genere una representación adecuada, ya que podemos caer en una &lt;em&gt;loss&lt;/em&gt; desbalanceada. Más adelante, dificultaremos la transmisión de mensajes añadiendo ruido a la representación intermedia, de forma que la salida del &lt;em&gt;encoder&lt;/em&gt; no sea exactamente igual a la entrada del &lt;em&gt;decoder&lt;/em&gt;, así conseguiremos perturbar el sistema y no conseguir \( L_{decoder} = L_{CE} = 0 \) tras unos pocos pasos de entrenamiento.&lt;/p&gt;

&lt;p&gt;Solo restaría elegir una estrategia de entrenamiento. Como se ha descrito más arriba, cada sujeto está formado por dos redes con cometidos diferentes y cada comunicación enfrenta partes distintas de cada sujeto. Por lo tanto, con \( N \) sujetos tendremos \( 2N \) redes y \( N^2 \) formas de enfrentarlas (contando con que queremos que un sujeto pueda entenderse a sí mismo). Para entrenar a todos los sujetos a la vez y evitar que algunas redes adquieran más nivel que otras tendremos que seguir una estrategia escalonada, alternando los pasos de entrenamiento entre todas las combinaciones. Así, tanto el tiempo como la complejidad del entrenamiento crecen con \( \mathcal{O}(N^2) \).&lt;/p&gt;

&lt;p&gt;Como creo que está quedando muy largo, mejor seguimos en otro post, por lo que cerramos aquí la primera parte de este artículo sobre cómo idear un sistema de redes neuronales del que pueda emerger un lenguaje común y cómo hacer que este lenguaje tenga la forma que queramos. En la segunda parte se explorarán los resultados y otras estrategias de entrenamiento en función de estos. Además, se incluirá una fuente de ruido para hacer el entrenamiento más robusto.&lt;/p&gt;

&lt;hr /&gt;

&lt;hr /&gt;

&lt;ol class=&quot;bibliography&quot;&gt;&lt;li&gt;&lt;span id=&quot;Brown2020&quot;&gt;Brown, Tom B., et al. &lt;i&gt;Language Models Are Few-Shot Learners&lt;/i&gt;. May 2020, https://arxiv.org/abs/2005.14165.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Shannon1948&quot;&gt;Shannon, C. E. “A Mathematical Theory of Communication.” &lt;i&gt;Bell System Technical Journal&lt;/i&gt;, 1948, doi:10.1002/j.1538-7305.1948.tb01338.x.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Neil2012&quot;&gt;Neil, Carlson. “Physiology of Behavior.” &lt;i&gt;IEEE Transactions on Information Theory&lt;/i&gt;, 2012.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Bray2015&quot;&gt;Bray, Adam, et al. &lt;i&gt;Star Wars: Absolutely Everything You Need to Know&lt;/i&gt;. DK Children, 2015, p. 200.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Krizhevsky2012&quot;&gt;Krizhevsky, Alex, et al. “ImageNet Classification with Deep Convolutional Neural Networks.” &lt;i&gt;Advances in Neural Information Processing Systems&lt;/i&gt;, 2012, doi:10.1061/(ASCE)GT.1943-5606.0001284.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;He2016&quot;&gt;He, Kaiming, et al. “Deep Residual Learning for Image Recognition.” &lt;i&gt;Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition&lt;/i&gt;, 2016, doi:10.1109/CVPR.2016.90.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;DotCSV2020&quot;&gt;Dot CSV. &lt;i&gt;¿Qué Es Una Red Neuronal Convolucional? Los OJOS De La Inteligencia Artificial - YouTube&lt;/i&gt;. 2020, https://www.youtube.com/watch?v=V8j1oENVz00.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Cs231n&quot;&gt;Stanford, Cs231n. &lt;i&gt;CS231n Convolutional Neural Networks for Visual Recognition&lt;/i&gt;. https://cs231n.github.io/convolutional-networks/. Accessed December 21, 2020.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Gomez2018&quot;&gt;Gómez, Raúl. &lt;i&gt;Understanding Categorical Cross-Entropy Loss, Binary Cross-Entropy Loss, Softmax Loss, Logistic Loss, Focal Loss and All Those Confusing Names&lt;/i&gt;. May 2018, https://gombru.github.io/2018/05/23/cross_entropy_loss/.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Gatys2015&quot;&gt;Gatys, Leon, et al. “A Neural Algorithm of Artistic Style.” &lt;i&gt;Journal of Vision&lt;/i&gt;, 2015, doi:10.1167/16.12.326.&lt;/span&gt;&lt;/li&gt;&lt;/ol&gt;
&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;En realidad el área de Wernicke y el área de Broca están conectadas por el fascículo arqueado &lt;a class=&quot;citation&quot; href=&quot;#Neil2012&quot;&gt;(Neil)&lt;/a&gt;. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Solo se muestra la magnitud en este espectrograma. Para calcularlo hay que tomar la transformada de Fourier de la señal, y eso lleva aparejada una fase que no se menciona aquí pero que es fundamental para hacer la transformada inversa después. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Esto aplica en modelos supervisados, en los que conocemos el valor que debería predecir el modelo para cada valor de entrada. Existen otros tipos de algoritmos (no supervisados, por ejemplo), en los que esto no se cumple y la función de &lt;em&gt;loss&lt;/em&gt; adquiere otras formas. &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name></name></author><category term="IA" /><category term="IA" /><category term="imagen" /><category term="lenguaje" /><summary type="html">La pregunta que encabeza este post es recurrente en todos los círculos de la Inteligencia Artificial. Aquellos que trabajan en NLP (procesamiento de lenguaje natural por sus siglas en inglés) buscan concebir sistemas que sean capaces de entender -procesar- lenguaje producido por seres humanos y tomar acciones acordes. GPT-3 es un ejemplo de esto, el cometido de este modelo tan solo es predecir (aunque muy acertadamente) la palabra más probable que seguirá una secuencia de texto (Brown et al.). Sin embargo, por cómo ha sido entrenado, en ningún caso se puede defender que el modelo posea una comprensión profunda de las palabras que está produciendo ni que tenga un objetivo en particular al expresarlas. Si se me permite mi opinión, creo que el salto que nos queda por delante hasta cubrir este último punto es todavía enorme y no vendrá en los próximos años. Al fin y al cabo, todavía no entendemos cómo funciona la mente humana. Así que la respuesta a la pregunta del encabezado será… “depende”, depende de a lo que nos refiramos con la palabra idioma.</summary></entry></feed>