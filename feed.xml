<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="https://www.javiergamazo.com//blog/feed.xml" rel="self" type="application/atom+xml" /><link href="https://www.javiergamazo.com//blog/" rel="alternate" type="text/html" /><updated>2021-01-25T08:09:04+00:00</updated><id>https://www.javiergamazo.com//blog/feed.xml</id><title type="html">DidYouKnow</title><subtitle>The place where I talk about anything</subtitle><entry><title type="html">Can an AI make up a language?</title><link href="https://www.javiergamazo.com//blog/ai/2020/12/25/discussing-networks_en.html" rel="alternate" type="text/html" title="Can an AI make up a language?" /><published>2020-12-25T09:00:00+00:00</published><updated>2020-12-25T09:00:00+00:00</updated><id>https://www.javiergamazo.com//blog/ai/2020/12/25/discussing-networks_en</id><content type="html" xml:base="https://www.javiergamazo.com//blog/ai/2020/12/25/discussing-networks_en.html">&lt;p&gt;The question in the header is recurring in all AI circles. Those who work on NLP (Natural Language Processing) strive to develop systems capable of processing language produced by human beings and take according actions. GPT-3 is an example of this, the goal of this model is just to predict (although very accurately) the most likely word following a text sequence &lt;a class=&quot;citation&quot; href=&quot;#Brown2020&quot;&gt;(Brown et al.)&lt;/a&gt;. However, due to its training process, one cannot defend that the model has a deep comprehension of the words it is producing nor that it has an objective in mind by expressing them. I believe the leap we as human beings have ahead to cover this last hole is still huge and it will not come in the following years. After all, we still do not understand how human mind works. So the answer to the question in the header will be “it depends”. It depends on what we are referring to when we say “language”.&lt;/p&gt;

&lt;p&gt;It is therefore relevant to know what we mean by “language” when we talk about it in this post. We will be referring to a common language shared by several systems through which they understand each other and transmit ideas. However, these ideas will come predefined by an agent external to the AI. A basic example could be to transmit a random number from 0 to 9 from system A to B in a common language, that B would process it, calculate its double, return it to A and A would receive the correct answer (the double of the first number). In this post we’ll try to lay the foundation for a project that meets all those requirements and one more condition: that the language shared by both systems is similar to that of the Star Wars R2-D2 Droid.&lt;/p&gt;

&lt;h3 id=&quot;it-takes-two-to-tango&quot;&gt;It takes two to tango&lt;/h3&gt;

&lt;p&gt;Indeed, for communication to exist there must be at least two systems: a transmitter of information and a receiver. Following the Communication Theory of Claude E. Shannon &lt;a class=&quot;citation&quot; href=&quot;#Shannon1948&quot;&gt;(Shannon)&lt;/a&gt;, it is also necessary a source of information, a channel, a message, a destination and a noise source. While the precise definition of these components is left to the reader’s discretion, it is of paramount importance for this project to distinguish between &lt;span style=&quot;color:blue&quot;&gt;information source&lt;/span&gt; and &lt;span style=&quot;color:red&quot;&gt;transmitter&lt;/span&gt;, and &lt;span style=&quot;color:blue&quot;&gt;destination&lt;/span&gt; and &lt;span style=&quot;color: red&quot;&gt;receiver&lt;/span&gt;: when we talk about transmitter we refer to the technical system in charge of encoding the information source to a set of signals suitable for the channel (and vice versa for the receiver, which decodes it so that the receiver can use the information). Applied to verbal communication between two people A and B, the source of information will be A’s idea, whose brain will be in charge of coding it in the form of words - message - that will be sent through the air - channel - until it reaches B. B’s brain will decode the words and generate an idea, which in the best case will be the same as the idea of A’s brain.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/1.discussing/images/information_theory_diagram_en.png&quot; alt=&quot;Information theory&quot; class=&quot;image-centered&quot; width=&quot;700px&quot; /&gt;
&lt;em&gt;Figure 1: Shannon’s Information Theory diagram. Adapted from &lt;a class=&quot;citation&quot; href=&quot;#Shannon1948&quot;&gt;(Shannon)&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Let us now transform this theory into the field of AI. As we know, neural networks are tremendously effective in generating representations from data, so we could face two of them (subjects A and B) and make them share messages. A would encode the message and B would… decode it? And what would happen if B wanted to transmit an idea to A? To find the solution to this question we can look at the most efficient system we know when it comes to producing and processing language: the brain. This organ takes care of both functions simultaneously (or can’t you listen to music and talk at the same time?), and that is because the information flow passes through separate areas&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;. These two zones are Wernicke’s area and Broca’s area, which are in charge of understanding and producing language respectively &lt;a class=&quot;citation&quot; href=&quot;#Neil2012&quot;&gt;(Neil)&lt;/a&gt;. Following this example, it seems logical to make our subjects be formed by two different and disconnected networks. One of these networks, the &lt;em&gt;encoder&lt;/em&gt; will be in charge of producing the language while the &lt;em&gt;decoder&lt;/em&gt; will try to understand it. In case A wants to transmit an idea to B, A’s &lt;em&gt;encoder&lt;/em&gt; will be responsible for transforming this idea into a common language and B’s &lt;em&gt;decoder&lt;/em&gt; will interpret it.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/1.discussing/images/estructura.png&quot; alt=&quot;Structure&quot; width=&quot;500px&quot; class=&quot;image-centered&quot; /&gt;
&lt;em&gt;Figure 2: Structure of a system with two subjects.&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;talking-as-clear-as-r2-d2&quot;&gt;Talking as clear as R2-D2&lt;/h3&gt;

&lt;p&gt;I will first describe R2-D2’s way of communicating for those who do not know it. It is a series of beeps, whistles and other sounds agglutinated to form something that resembles phrases &lt;a class=&quot;citation&quot; href=&quot;#Bray2015&quot;&gt;(Bray et al.)&lt;/a&gt;. On YouTube there are &lt;a href=&quot;https://www.youtube.com/watch?v=2-BKjnAgNgY&quot; target=&quot;_blank&quot;&gt;several&lt;/a&gt; videos showing these sounds. We will try to make the neural networks imitate this language when speaking, forcing the representation of the idea by the encoder to also be a series of beeps. However, forcing this type of representation is not trivial for two reasons: first, language must be created spontaneously through conversation between the networks, there must be no human interaction in this process. Secondly, we must define what we mean by “representation” in this context since language is very varied. We will address the second question first, which will lead us inexorably to the solution of the first._&lt;/p&gt;

&lt;p&gt;A sound can be represented in three different ways depending on what we want to know about it:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;The temporal representation&lt;/strong&gt; shows the intensity of the sound as a function of time.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Representation in frequencies:&lt;/strong&gt; explained very briefly, all sound, because it is a wave as a function of time, can be broken down into more basic waves with different frequencies. The method by which we go from a wave in the time domain to a wave in the frequency domain is called &lt;em&gt;Fourier transform&lt;/em&gt;. The representation in frequencies contains information on how important (amplitude) each frequency is in giving rise to the underlying sound.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Representation in the form of a spectrogram:&lt;/strong&gt; the two previous representations have two dimensions, i.e. amplitude as a function of time or amplitude of each frequency. However, we can join both to produce a three-dimensional representation: time, amplitude and frequency. A spectrogram shows the evolution of frequency and intensity in time. Typically, intensity is defined by color, while time takes the abscissa axis and frequency the ordinate axis. Unlike the previous cases, a spectrogram can be saved as an image since the relevant information is coded both in the color of the pixels and in their position. The following figure shows the spectrogram of a voice fragment of R2-D2&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/1.discussing/images/espectrograma.png&quot; alt=&quot;Spectrogram&quot; class=&quot;image-centered&quot; /&gt;
&lt;em&gt;Figure 3: Spectrogram of a sound produced by R2-D2. It has been calculated by taking the Fourier transform of all the audio of the video in &lt;a href=&quot;https://www.youtube.com/watch?v=2-BKjnAgNgY&quot;&gt;https://www.youtube.com/watch?v=2-BKjnAgNgY&lt;/a&gt; and dividing the result in 256 pixels images.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;It has been shown that neural networks operate well with tabulated data and especially well with images &lt;a class=&quot;citation&quot; href=&quot;#Krizhevsky2012&quot;&gt;(Krizhevsky et al.)&lt;/a&gt;&lt;a class=&quot;citation&quot; href=&quot;#He2016&quot;&gt;(He et al.)&lt;/a&gt;, so it seems logical to use the spectrogram for this task. In general, convolutional neural networks are used to work with images because they are capable of capture relations in their pixels, usually by identifying borders in different directions or other coarser features like textures. Explaining in detail how this type of networks work would be useful for another article, so it is left to the interested reader to become familiar with this type of architectures, since there are many resources available (&lt;a class=&quot;citation&quot; href=&quot;#Cs231n&quot;&gt;(Stanford)&lt;/a&gt;, for example). I will only say here that with a network of convolutional layers we can obtain a multi-dimensional representation of the input data (an image). That is, we can get the translation of any number in an image, and this translation can be learned for a concrete task.&lt;/p&gt;

&lt;h3 id=&quot;a-spoonful-of-architecture&quot;&gt;A spoonful of architecture&lt;/h3&gt;

&lt;p&gt;Once all the technical aspects of the problem have been addressed, we will focus on defining the system architecture and the training process. Determining network architecture is usually more of an art than a science, so I will not go into detail. First, we will code the idea (the number) in a One-Hot way, which means that to transmit a digit from \( N \) possible ones we will use a vector of  \( N \) components with zero in all of them and one in the relevant component. For example, the vector for the number 1 in the interval \( [0, 4] \) will be \([0, 1, 0, 0, 0]\). To use this data with convolutional networks we will have to transform it to three dimensions ( (C\times H\times W ) \), increasing first the amount of components with at least one fully connected layer and resizing the result. Later, we will add convolutional layers so that the result is a map of the dimensions of the spectrogram we are looking for. As the task we are trying to solve is symmetric (the encoder and the decoder must perform opposite tasks), we will opt for symmetric architectures for these two networks. If we have said that the encoder will start with a fully connected layer followed by convolutional layers, the decoder will start with convolutional layers and finally another fully connected one. In this way, the diagram above takes on the following structure:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/1.discussing/images/estructura_espectrograma_en.png&quot; alt=&quot;Structure with spectrogram&quot; width=&quot;650px&quot; class=&quot;image-centered&quot; /&gt;
&lt;em&gt;Figure 3: Structure of a system with two subjects where number 1 is sent. By comparison with Shannon’s information theory, the central spectrogram shows the message. Only one part of the system is shown for the sake of simplicity.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;To make a neural network system behave in a certain way, one must specify a quantity to minimize. This amount is called &lt;em&gt;loss&lt;/em&gt;, and in many cases it is a function that depends on the network output and the original data&lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;. Applied to our problem, we can immediately define a quantity that the system should minimize: we want B to interpret the same idea that A is trying to transmit. As there is only one correct number each time, the problem is multiclass (predict a number and only one of ten possible) and we will minimize the &lt;em&gt;Cross-Entropy loss&lt;/em&gt;, defined as:
\[ L_{CE} = - \sum_i^C t_i \log(s_i),\]
where \( C \) are the possible classes and \( t_i \) and \( s_i \) represent the real and predicted labels, respectively&lt;a class=&quot;citation&quot; href=&quot;#Gomez2018&quot;&gt;(Gómez)&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;By minimizing the Cross-Entropy loss we get the two networks to agree on the number they are transmitting. It is important to point out that only by minimizing this function the networks would create an internal language at the output of the encoder, but this would very likely be random. As we want them to use R2-D2’s voice, we will have to add a function of loss more and we will do it… with style.&lt;/p&gt;

&lt;h3 id=&quot;speaking-with-style&quot;&gt;Speaking with style&lt;/h3&gt;

&lt;p&gt;As explained above, language must be created spontaneously and without human interaction, and for this purpose representation in the form of a spectrogram can help us. As it is evident, we cannot force the output of the encider to be equal (pixel by pixel) to a concrete spectrogram or to a group of these since we would be violating the second assumption of the problem (there would exist in that case strong human interaction). We will need then a less intrusive loss function, that does not look for differences by pixel but something more general. Fortunately, in 2015 a type of loss was introduced that is suitable for this use case: the style loss &lt;a class=&quot;citation&quot; href=&quot;#Gatys2015&quot;&gt;(Gatys et al.)&lt;/a&gt;. Intuitively, this loss is calculated by passing the images through a pre-trained neural network (VGG) and comparing the intermediate representations of the target image and the generated image. The authors apply this idea to transfer the style of a picture to a photo, keeping the underlying idea of the original photo intact. For anyone interested, mathematically it looks like this with \( l \) intermediate representations:&lt;/p&gt;

&lt;p&gt;\[ L_{style} = \sum_l w^lL_{style}^l,   \]
\[ L_{style}^l = \dfrac{1}{M^l} \sum_{ij} (G_{ij}^l(s) - G_{ij}^l(g))^2,   \]
where \( G(s) \) and \( G(g) \) refer to the Gram matrix of the style image and the generated image. For further information, I think &lt;a href=&quot;https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-neural-style-transfer-ef88e46697ee&quot;&gt;this article&lt;/a&gt; can be interesting.&lt;/p&gt;

&lt;p&gt;We can use the style loss to induce the output of the encoder to be similar to the reference spectrograms. This will presumably keep the sounds random but the result will remind us of the movie droid. However, this function cannot be applied to both encoder and decoder. Since only the encoder is involved in the language creation process, it is the only one that should receive feedback in this regard. However, the idea must flow through both networks, so the Cross-Entropy loss must spread throughout the system. Thus, the functions in each case result:&lt;/p&gt;

&lt;p&gt;\[ L_{decoder} = L_{CE} = - \sum_i^C t_i \log(s_i),\]
\[ L_{encoder} = L_{CE} + \lambda L_{style} = - \sum_i^C t_i \log(s_i) + \lambda \sum_l w^l \dfrac{1}{M^l} \sum_{ij} (G_{ij}^l(s) - G_{ij}^l(g))^2\]&lt;/p&gt;

&lt;p&gt;And the whole system is:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/1.discussing/images/estructura_loss_en.png&quot; alt=&quot;Structure with loss&quot; width=&quot;650px&quot; class=&quot;image-centered&quot; /&gt;
&lt;em&gt;Figure 4: Propagation and computation of the loss function throughout the system. Black: forward propagation. Red: backpropagation of the loss function.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;One has to notice that the simplicity of the message we are transmitting can be a problem in order to train the encoder to generate an adequate representation, as it may cause the loss function to be unbalanced. Later on, we will make transmission harder with the addition of noise to the intermediate representation, thus making the input of the decoder different from the output of the encoder, perturbing the system and avoiding \(L_{decoder} = L_{CE} = 0 \) after a couple of training steps.&lt;/p&gt;

&lt;p&gt;All that remains is choosing a training strategy. As described above, each subject is formed by two networks with different tasks and each communication faces different parts of each subject. Therefore, with \( N \) subjects we will have  \( 2N \) networks and  \( N^2 \) ways to face them (counting on the fact that we want a subject to be able to understand himself). To train all the subjects at the same time and avoid that some nets acquire more level than others we will have to follow a staggered strategy, alternating the training steps between all the combinations. Thus, both the time and the complexity of the training grow with \( \mathcal{O}(N^2) \). This strategy will also prevent the system from learning \( N^2 \) different languages, as all the networks will be learning in parallel.&lt;/p&gt;

&lt;p&gt;Thus concludes the first part of this article on how to devise a system of neural networks from which a common language can emerge and how to make this language have the form we want. The second part will explore the results and other training strategies based on these. In addition, a noise source will be included to make the training more robust.&lt;/p&gt;

&lt;hr /&gt;

&lt;hr /&gt;

&lt;ol class=&quot;bibliography&quot;&gt;&lt;li&gt;&lt;span id=&quot;Brown2020&quot;&gt;Brown, Tom B., et al. &lt;i&gt;Language Models Are Few-Shot Learners&lt;/i&gt;. May 2020, https://arxiv.org/abs/2005.14165.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Shannon1948&quot;&gt;Shannon, C. E. “A Mathematical Theory of Communication.” &lt;i&gt;Bell System Technical Journal&lt;/i&gt;, 1948, doi:10.1002/j.1538-7305.1948.tb01338.x.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Neil2012&quot;&gt;Neil, Carlson. “Physiology of Behavior.” &lt;i&gt;IEEE Transactions on Information Theory&lt;/i&gt;, 2012.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Bray2015&quot;&gt;Bray, Adam, et al. &lt;i&gt;Star Wars: Absolutely Everything You Need to Know&lt;/i&gt;. DK Children, 2015, p. 200.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Krizhevsky2012&quot;&gt;Krizhevsky, Alex, et al. “ImageNet Classification with Deep Convolutional Neural Networks.” &lt;i&gt;Advances in Neural Information Processing Systems&lt;/i&gt;, 2012, doi:10.1061/(ASCE)GT.1943-5606.0001284.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;He2016&quot;&gt;He, Kaiming, et al. “Deep Residual Learning for Image Recognition.” &lt;i&gt;Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition&lt;/i&gt;, 2016, doi:10.1109/CVPR.2016.90.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Cs231n&quot;&gt;Stanford, Cs231n. &lt;i&gt;CS231n Convolutional Neural Networks for Visual Recognition&lt;/i&gt;. https://cs231n.github.io/convolutional-networks/. Accessed December 21, 2020.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Gomez2018&quot;&gt;Gómez, Raúl. &lt;i&gt;Understanding Categorical Cross-Entropy Loss, Binary Cross-Entropy Loss, Softmax Loss, Logistic Loss, Focal Loss and All Those Confusing Names&lt;/i&gt;. May 2018, https://gombru.github.io/2018/05/23/cross_entropy_loss/.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Gatys2015&quot;&gt;Gatys, Leon, et al. “A Neural Algorithm of Artistic Style.” &lt;i&gt;Journal of Vision&lt;/i&gt;, 2015, doi:10.1167/16.12.326.&lt;/span&gt;&lt;/li&gt;&lt;/ol&gt;
&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Actually the Wernicke area and the Broca area are connected by the arcuate fasciculus &lt;a class=&quot;citation&quot; href=&quot;#Neil2012&quot;&gt;(Neil)&lt;/a&gt;. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Only the magnitude is shown in this spectrogram. To calculate it, the Fourier transform of the signal has to be computed, and that carries a phase that is not mentioned here but that is fundamental to make the inverse transform later. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;This applies for supervised models, where we know the value the model should predict for each input value. There are other types of algorithms (unsupervised, for example), in which this is not fulfilled and the loss function takes other forms. &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name></name></author><category term="AI" /><category term="AI" /><category term="image" /><category term="language" /><summary type="html">The question in the header is recurring in all AI circles. Those who work on NLP (Natural Language Processing) strive to develop systems capable of processing language produced by human beings and take according actions. GPT-3 is an example of this, the goal of this model is just to predict (although very accurately) the most likely word following a text sequence (Brown et al.). However, due to its training process, one cannot defend that the model has a deep comprehension of the words it is producing nor that it has an objective in mind by expressing them. I believe the leap we as human beings have ahead to cover this last hole is still huge and it will not come in the following years. After all, we still do not understand how human mind works. So the answer to the question in the header will be “it depends”. It depends on what we are referring to when we say “language”.</summary></entry><entry><title type="html">¿Puede una IA inventar un idioma?</title><link href="https://www.javiergamazo.com//blog/ia/2020/12/25/discussing-networks_es.html" rel="alternate" type="text/html" title="¿Puede una IA inventar un idioma?" /><published>2020-12-25T09:00:00+00:00</published><updated>2020-12-25T09:00:00+00:00</updated><id>https://www.javiergamazo.com//blog/ia/2020/12/25/discussing-networks_es</id><content type="html" xml:base="https://www.javiergamazo.com//blog/ia/2020/12/25/discussing-networks_es.html">&lt;p&gt;La pregunta que encabeza este post es recurrente en todos los círculos de la Inteligencia Artificial. Aquellos que trabajan en NLP (procesamiento de lenguaje natural por sus siglas en inglés) buscan concebir sistemas que sean capaces de entender -procesar- lenguaje producido por seres humanos y tomar acciones acordes. GPT-3 es un ejemplo de esto, el cometido de este modelo tan solo es predecir (aunque muy acertadamente) la palabra más probable que seguirá una secuencia de texto &lt;a class=&quot;citation&quot; href=&quot;#Brown2020&quot;&gt;(Brown et al.)&lt;/a&gt;. Sin embargo, por cómo ha sido entrenado, en ningún caso se puede defender que el modelo posea una comprensión profunda de las palabras que está produciendo ni que tenga un objetivo en particular al expresarlas. Si se me permite mi opinión, creo que el salto que nos queda por delante hasta cubrir este último punto es todavía enorme y no vendrá en los próximos años. Al fin y al cabo, todavía no entendemos cómo funciona la mente humana. Así que la respuesta a la pregunta del encabezado será… “depende”, depende de a lo que nos refiramos con la palabra idioma.&lt;/p&gt;

&lt;p&gt;Es por tanto relevante saber a qué nos referimos con “idioma” o “lenguaje” cuando hablemos de ello en este post. Nos estaremos refiriendo a un lenguaje común que compartan varios sistemas a través del cual puedan llegar a entenderse y transmitir ideas. No obstante, estas ideas vendrán predefinidas por un agente externo a la IA. Un ejemplo básico podría ser transmitir un número aleatorio de 0 a 9 del sistema A al B en un lenguaje común, que el B lo procesara, calculara su doble, lo devolviera a A y A interpretara el resultado correcto. En este post intentaremos sentar las bases de un proyecto que cumpla todos esos requisitos y una condición más: que el idioma que compartan ambos sistemas sea similar al del droide R2-D2 de Star Wars.&lt;/p&gt;

&lt;h3 id=&quot;dos-no-hablan-si-uno-no-quiere&quot;&gt;Dos no hablan si uno no quiere&lt;/h3&gt;

&lt;p&gt;Efectivamente, para que exista comunicación debe haber al menos dos sistemas: un emisor de información (transmisor) y otro receptor de la misma. Siguiendo la Teoría de la Comunicación de Claude E. Shannon &lt;a class=&quot;citation&quot; href=&quot;#Shannon1948&quot;&gt;(Shannon)&lt;/a&gt;, es necesario además una fuente de información, un canal, un mensaje, un destinatario y un elemento de ruido. Si bien la definición precisa de estos componentes se deja a discreción del lector, es de capital importancia para este proyecto distinguir entre &lt;span style=&quot;color:blue&quot;&gt;fuente de información&lt;/span&gt; y &lt;span style=&quot;color:red&quot;&gt;transmisor&lt;/span&gt;, y &lt;span style=&quot;color:blue&quot;&gt;destinatario&lt;/span&gt; y &lt;span style=&quot;color:red&quot;&gt;receptor&lt;/span&gt;: cuando hablamos de transmisor nos referimos al sistema técnico encargado de &lt;em&gt;codificar&lt;/em&gt; la fuente de información a un conjunto de señales aptas para el canal (y viceversa para el receptor, que la &lt;em&gt;decodifica&lt;/em&gt; para que el destinatario pueda utilizar la información). Aplicado a la comunicación verbal entre dos personas A y B, la fuente de información será la idea de A, cuyo cerebro se encargará de codificar en forma de palabras -mensaje- que se enviarán por el aire -canal- hasta llegar a B. El cerebro de B decodificará las palabras y generará una idea, que en el mejor de los casos será igual a la idea del cerebro de A.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/1.discussing/images/information_theory_diagram_es.png&quot; alt=&quot;Information theory&quot; class=&quot;image-centered&quot; width=&quot;700px&quot; /&gt;
&lt;em&gt;Figura 1: Diagrama de la teoría de la información de Shannon. Adaptado de &lt;a class=&quot;citation&quot; href=&quot;#Shannon1948&quot;&gt;(Shannon)&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Transformemos ahora esta teoría al campo de la IA. Como sabemos, las redes neuronales son tremendamente eficaces a la hora de generar representaciones a partir de datos, por lo que podríamos enfrentar dos de ellas (sujetos A y B) y hacerlas compartir mensajes. A codificaría el mensaje y B lo… ¿decodificaría? ¿Y qué pasaría si B quisiera transmitir una idea a A? Para encontrar la solución a esta pregunta podemos fijarnos en el sistema más eficiente que conocemos a la hora de producir y procesar lenguaje: el cerebro. Este órgano se encarga de ambas funciones simultáneamente (¿o es que no puedes escuchar música y hablar a la vez?), y eso se debe a que el flujo de información transcurre por zonas separadas&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;. Estas dos zonas son el área de Wernicke y el área de Broca, que se encargan de la comprensión y de la producción del lenguaje respectivamente &lt;a class=&quot;citation&quot; href=&quot;#Neil2012&quot;&gt;(Neil)&lt;/a&gt;. Siguiendo este ejemplo, parece lógico hacer que nuestros sujetos estén formados por dos redes diferentes y desconectadas. Una de estas redes, el &lt;em&gt;encoder&lt;/em&gt; se encargará de producir el lenguaje mientras que el &lt;em&gt;decoder&lt;/em&gt; tratará de comprenderlo. En caso de que A quiera transmitir una idea a B, será el &lt;em&gt;encoder&lt;/em&gt; de A el responsable de transformar esta idea a un lenguaje común y el &lt;em&gt;decoder&lt;/em&gt; de B lo interpretará.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/1.discussing/images/estructura.png&quot; alt=&quot;Estructura&quot; width=&quot;500px&quot; class=&quot;image-centered&quot; /&gt;
&lt;em&gt;Figura 2: Estructura de un sistema con dos individuos.&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;hablemos-tan-claro-como-r2-d2&quot;&gt;Hablemos tan claro como R2-D2&lt;/h3&gt;

&lt;p&gt;Describiré en primer lugar la forma de comunicarse de R2-D2 para aquellos que la desconozcan. Se trata de una serie de pitidos, silbidos y otros sonidos aglutinados para formar algo que se asemeja a frases &lt;a class=&quot;citation&quot; href=&quot;#Bray2015&quot;&gt;(Bray et al.)&lt;/a&gt;. En YouTube hay &lt;a href=&quot;https://www.youtube.com/watch?v=2-BKjnAgNgY&quot; target=&quot;_blank&quot;&gt;infinidad&lt;/a&gt; de vídeos mostrando estos sonidos. Intentaremos hacer que las redes neuronales imiten este lenguaje a la hora de hablar, forzando que la representación de la idea por parte del &lt;em&gt;encoder&lt;/em&gt; sea también una serie de pitidos. Sin embargo, forzar este tipo de representación no es trivial por dos motivos: en primer lugar, el lenguaje debe ser creado espontáneamente a través de la conversación entre las redes, no debe existir ningún tipo de interacción humana en este proceso. En segundo lugar, debemos definir a qué nos referimos con “representación” en este contexto puesto que el lenguaje es muy variado. Abordaremos primero la segunda cuestión, lo que nos llevará inexorablemente hasta la solución de la primera.&lt;/p&gt;

&lt;p&gt;Un sonido puede ser representado de tres formas diferentes en función de lo que busquemos conocer acerca del mismo:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Representación temporal:&lt;/strong&gt; muestra la intensidad del sonido en función del tiempo.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Representación en frecuencias:&lt;/strong&gt; explicado muy brevemente, todo sonido, por el hecho de ser una onda en función del tiempo, puede ser descompuesto en ondas más básicas con diferentes frecuencias. El método con el que pasamos de una onda en el dominio temporal a una onda en el dominio de frecuencias recibe el nombre de &lt;em&gt;transformada de Fourier&lt;/em&gt;. La representación en frecuencias contiene información de lo importante (amplitud) que es cada frecuencia para dar lugar al sonido subyacente.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Representación en forma de espectrograma:&lt;/strong&gt; las dos representaciones anteriores tienen dos dimensiones, i.e. amplitud en función del tiempo o amplitud de cada frecuencia. Sin embargo, podemos unir ambas para dar lugar a una representación tridimensional: tiempo, amplitud y frecuencia. Un espectrograma muestra la evolución de la frecuencia y de la intensidad en el tiempo. Típicamente, la intensidad se define por el color, mientras que el tiempo toma el eje de abscisas y la frecuencia, el de ordenadas. A diferencia de en los casos anteriores, un espectrograma puede ser guardado en forma de imagen ya que la informaciónm relevante está codificada tanto en el color de los píxeles como en su posición. La siguiente figura muestra el espectrograma de un fragmento de la voz de R2-D2&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/1.discussing/images/espectrograma.png&quot; alt=&quot;Espectrograma&quot; class=&quot;image-centered&quot; /&gt;
&lt;em&gt;Figura 3: Espectrograma de un sonido producido por R2-D2. Se ha calculado tomando la transformada de Fourier de todo el audio del vídeo en &lt;a href=&quot;https://www.youtube.com/watch?v=2-BKjnAgNgY&quot;&gt;https://www.youtube.com/watch?v=2-BKjnAgNgY&lt;/a&gt; y dividiendo el resultado en imágenes de 256 píxeles.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Se ha demostrado que las redes neuronales operan bien con datos tabulados y especialmente bien con imágenes &lt;a class=&quot;citation&quot; href=&quot;#Krizhevsky2012&quot;&gt;(Krizhevsky et al.)&lt;/a&gt;&lt;a class=&quot;citation&quot; href=&quot;#He2016&quot;&gt;(He et al.)&lt;/a&gt;, así que parece lógico utilizar el espectograma para esta tarea. En general, para trabajar con imágenes se utilizan redes neuronales convolucionales porque son capaces de capturar relaciones en ellas, normalmente a través de la identificación de bordes en diferentes direcciones y otras características más vagas como las texturas. Explicar en detalle cómo funcionan este tipo de redes daría para otro artículo, por lo que se deja al lector interesado el familiarizarse con este tipo de arquitecturas, ya que existen multitud de recursos disponibles (&lt;a class=&quot;citation&quot; href=&quot;#DotCSV2020&quot;&gt;(Dot CSV)&lt;/a&gt;, &lt;a class=&quot;citation&quot; href=&quot;#Cs231n&quot;&gt;(Stanford)&lt;/a&gt; [en inglés], por ejemplo). Tan solo diré aquí que con una red de capas convolucionales podemos obtener una representación en varias dimensiones de los datos de entrada (una imagen). Es decir, podemos conseguir la traducción de un número cualquiera en una imagen, y esta traducción puede ser aprendida para una tarea concreta.&lt;/p&gt;

&lt;h3 id=&quot;un-poco-de-arquitectura&quot;&gt;Un poco de arquitectura&lt;/h3&gt;

&lt;p&gt;Una vez abordados todos los aspectos técnicos del problema nos centraremos en definir la arquitectura del sistema y la forma de entrenamiento. Determinar la arquitectura de las redes generalmente tiene más de arte que de ciencia, por lo que no me explayaré en los detalles. En primer lugar, codificaremos la idea (el número) de forma &lt;em&gt;One-Hot&lt;/em&gt;, lo que significa que para transmitir un dígito de entre \( N \) posibles utilizaremos un vector de \( N \) componentes con cero en todas ellas y un uno en la componente relevante. Por ejemplo, el vector para el número 1 en el intervalo \( [0, 4] \) será \([0, 1, 0, 0, 0]\). Para utilizar estos datos con redes convolucionales deberemos transformarlos a tres dimensiones \( (C\times H\times W ) \), aumentando en primer lugar la cantidad de componentes con al menos una capa &lt;em&gt;fully connected&lt;/em&gt; y redimensionando el resultado. Posteriormente, añadiremos capas convolucionales de forma que el resultado sea un mapa de las dimensiones del espectrograma que buscamos. Como la tarea que buscamos resolver es simétrica (el &lt;em&gt;encoder&lt;/em&gt; y el &lt;em&gt;decoder&lt;/em&gt; deben realizar tareas opuestas), optaremos por arquitecturas simétricas para estas dos redes. Si hemos dicho que el &lt;em&gt;encoder&lt;/em&gt; empezará con una capa &lt;em&gt;fully connected&lt;/em&gt; seguida de capas convolucionales, el &lt;em&gt;decoder&lt;/em&gt; empezará por capas convolucionales y por último otra &lt;em&gt;fully connected&lt;/em&gt;. De esta forma, el diagrama anterior adquiere la siguiente estructura:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/1.discussing/images/estructura_espectrograma_es.png&quot; alt=&quot;Estructura con espectrograma&quot; width=&quot;650px&quot; class=&quot;image-centered&quot; /&gt;
&lt;em&gt;Figura 3: Estructura de un sistema con dos individuos en el que se envía el número 1. Por comparación con la teoría de la información de Shannon, el espectrograma central muestra el mensaje. Solo se muestra una parte del sistema (conexión de A con B) por simplicidad.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Para hacer que un sistema de redes neuronales se comporte de una forma determinada se debe especificar una cantidad a minimizar. A esta cantidad se le llama &lt;em&gt;loss&lt;/em&gt;, y en muchos casos es una función que depende de la salida de la red y de los datos originales&lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;. Aplicado a nuestro problema, podemos definir inmediatamente una cantidad que el sistema debe minimizar: queremos que B interprete la misma idea que está intentando transmitir A. Como solo hay un número correcto cada vez, el problema es multiclase (predecir un número y solo uno de entre diez posibles) y minimizaremos la &lt;em&gt;Cross-Entropy loss&lt;/em&gt;, definida como:
\[ L_{CE} = - \sum_i^C t_i \log(s_i),\]
donde \( C \) son las clases posibles y \( t_i \) y \( s_i \) representan la etiqueta real y la predicha por la red respectivamente &lt;a class=&quot;citation&quot; href=&quot;#Gomez2018&quot;&gt;(Gómez)&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Minimizando la &lt;em&gt;Cross-Entropy loss&lt;/em&gt; conseguimos que las dos redes se pongan de acuerdo en el número que están transmitiendo. Es importante puntualizar que tan solo minimizando esta cantidad las redes ya crearían un lenguaje interno a la salida del &lt;em&gt;encoder&lt;/em&gt;, pero éste sería probablemente aleatorio. Como queremos que utilicen la voz de R2-D2, tendremos que añadir una función de &lt;em&gt;loss&lt;/em&gt; más y lo haremos… con estilo.&lt;/p&gt;

&lt;h3 id=&quot;hablando-con-estilo&quot;&gt;Hablando con estilo&lt;/h3&gt;

&lt;p&gt;Como se ha explicado más arriba, el lenguaje debe ser creado espontáneamente y sin interacción humana, y para ello la representación en forma de espectrograma nos puede ayudar. Como es evidente, no podemos forzar que la salida del &lt;em&gt;encoder&lt;/em&gt; sea igual (píxel a píxel) a un espectrograma concreto ni a un grupo de estos puesto que estaríamos violando el segundo supuesto del problema (existiría en ese caso fuerte interacción humana). Necesitaremos entonces una función de &lt;em&gt;loss&lt;/em&gt; menos intrusiva, que no busque diferencias por píxel sino algo más general. Por suerte, en 2015 se introdujo un tipo de &lt;em&gt;loss&lt;/em&gt; idóneo para este caso de uso: la &lt;em&gt;loss&lt;/em&gt; de estilo &lt;a class=&quot;citation&quot; href=&quot;#Gatys2015&quot;&gt;(Gatys et al.)&lt;/a&gt;. Intuitivamente, esta &lt;em&gt;loss&lt;/em&gt; se calcula haciendo pasar las imágenes por una red neuronal preentrenada (VGG) y comparando las representaciones intermedias de la imagen objetivo y la imagen generada. Los autores aplican esta idea para transferir el estilo de un cuadro a una foto, manteniendo intacta la idea subyacente de la foto original. Para quien esté interesado, matemáticamente tiene este aspecto con \( l \) representaciones intermedias:&lt;/p&gt;

&lt;p&gt;\[ L_{estilo} = \sum_l w^lL_{estilo}^l,   \]
\[ L_{estilo}^l = \dfrac{1}{M^l} \sum_{ij} (G_{ij}^l(s) - G_{ij}^l(g))^2,   \]
donde \( G(s) \) y \( G(g) \) se refieren a la matriz de Gram de la imagen estilo y la imagen generada respectivamente. En esta &lt;em&gt;loss&lt;/em&gt; se está calculando la correlación entre las características extraídas en cada capa de la red preentrenada. Para más información, creo que &lt;a href=&quot;https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-neural-style-transfer-ef88e46697ee&quot;&gt;este artículo&lt;/a&gt; (en inglés) puede ser interesante.&lt;/p&gt;

&lt;p&gt;Podemos utilizar la &lt;em&gt;loss&lt;/em&gt; de estilo para inducir que la salida del &lt;em&gt;encoder&lt;/em&gt; sea similar a los espectrogramas de referencia. Con ello presumiblemente conseguiremos que los sonidos se mantengan aleatorios pero nos recuerden a los del droide de las películas. No obstante, esta función no puede ser aplicada a &lt;em&gt;encoder&lt;/em&gt; y &lt;em&gt;decoder&lt;/em&gt; por igual. Puesto que solo el &lt;em&gt;encoder&lt;/em&gt; participa en el proceso de creación del lenguaje, es el único que debe recibir &lt;em&gt;feedback&lt;/em&gt; en ese aspecto. Sin embargo, la idea debe fluir a través de ambas redes, por lo que la &lt;em&gt;Cross-Entropy loss&lt;/em&gt; se debe propagar por todo el sistema. Así, las funciones en cada caso resultan:
\[ L_{decoder} = L_{CE} = - \sum_i^C t_i \log(s_i),\]
\[ L_{encoder} = L_{CE} + \lambda L_{estilo} = - \sum_i^C t_i \log(s_i) + \lambda \sum_l w^l \dfrac{1}{M^l} \sum_{ij} (G_{ij}^l(s) - G_{ij}^l(g))^2\]&lt;/p&gt;

&lt;p&gt;Y el sistema total queda como en la Figura 4:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/1.discussing/images/estructura_loss_es.png&quot; alt=&quot;Estructura con loss&quot; width=&quot;650px&quot; class=&quot;image-centered&quot; /&gt;
&lt;em&gt;Figura 4: Propagación y cálculo de las funciones de _loss_ a través del sistema. En negro, propagación hacia delante. En rojo, propagación de las funciones de loss hacia atrás.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Hay que tener en cuenta que la simplicidad del mensaje que estamos transmitiendo puede suponer un problema a la hora de entrenar el &lt;em&gt;encoder&lt;/em&gt; para que genere una representación adecuada, ya que podemos caer en una &lt;em&gt;loss&lt;/em&gt; desbalanceada. Más adelante, dificultaremos la transmisión de mensajes añadiendo ruido a la representación intermedia, de forma que la salida del &lt;em&gt;encoder&lt;/em&gt; no sea exactamente igual a la entrada del &lt;em&gt;decoder&lt;/em&gt;, así conseguiremos perturbar el sistema y no conseguir \( L_{decoder} = L_{CE} = 0 \) tras unos pocos pasos de entrenamiento.&lt;/p&gt;

&lt;p&gt;Solo restaría elegir una estrategia de entrenamiento. Como se ha descrito más arriba, cada sujeto está formado por dos redes con cometidos diferentes y cada comunicación enfrenta partes distintas de cada sujeto. Por lo tanto, con \( N \) sujetos tendremos \( 2N \) redes y \( N^2 \) formas de enfrentarlas (contando con que queremos que un sujeto pueda entenderse a sí mismo). Para entrenar a todos los sujetos a la vez y evitar que algunas redes adquieran más nivel que otras tendremos que seguir una estrategia escalonada, alternando los pasos de entrenamiento entre todas las combinaciones. Así, tanto el tiempo como la complejidad del entrenamiento crecen con \( \mathcal{O}(N^2) \).&lt;/p&gt;

&lt;p&gt;Como creo que está quedando muy largo, mejor seguimos en otro post, por lo que cerramos aquí la primera parte de este artículo sobre cómo idear un sistema de redes neuronales del que pueda emerger un lenguaje común y cómo hacer que este lenguaje tenga la forma que queramos. En la segunda parte se explorarán los resultados y otras estrategias de entrenamiento en función de estos. Además, se incluirá una fuente de ruido para hacer el entrenamiento más robusto.&lt;/p&gt;

&lt;hr /&gt;

&lt;hr /&gt;

&lt;ol class=&quot;bibliography&quot;&gt;&lt;li&gt;&lt;span id=&quot;Brown2020&quot;&gt;Brown, Tom B., et al. &lt;i&gt;Language Models Are Few-Shot Learners&lt;/i&gt;. May 2020, https://arxiv.org/abs/2005.14165.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Shannon1948&quot;&gt;Shannon, C. E. “A Mathematical Theory of Communication.” &lt;i&gt;Bell System Technical Journal&lt;/i&gt;, 1948, doi:10.1002/j.1538-7305.1948.tb01338.x.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Neil2012&quot;&gt;Neil, Carlson. “Physiology of Behavior.” &lt;i&gt;IEEE Transactions on Information Theory&lt;/i&gt;, 2012.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Bray2015&quot;&gt;Bray, Adam, et al. &lt;i&gt;Star Wars: Absolutely Everything You Need to Know&lt;/i&gt;. DK Children, 2015, p. 200.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Krizhevsky2012&quot;&gt;Krizhevsky, Alex, et al. “ImageNet Classification with Deep Convolutional Neural Networks.” &lt;i&gt;Advances in Neural Information Processing Systems&lt;/i&gt;, 2012, doi:10.1061/(ASCE)GT.1943-5606.0001284.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;He2016&quot;&gt;He, Kaiming, et al. “Deep Residual Learning for Image Recognition.” &lt;i&gt;Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition&lt;/i&gt;, 2016, doi:10.1109/CVPR.2016.90.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;DotCSV2020&quot;&gt;Dot CSV. &lt;i&gt;¿Qué Es Una Red Neuronal Convolucional? Los OJOS De La Inteligencia Artificial - YouTube&lt;/i&gt;. 2020, https://www.youtube.com/watch?v=V8j1oENVz00.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Cs231n&quot;&gt;Stanford, Cs231n. &lt;i&gt;CS231n Convolutional Neural Networks for Visual Recognition&lt;/i&gt;. https://cs231n.github.io/convolutional-networks/. Accessed December 21, 2020.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Gomez2018&quot;&gt;Gómez, Raúl. &lt;i&gt;Understanding Categorical Cross-Entropy Loss, Binary Cross-Entropy Loss, Softmax Loss, Logistic Loss, Focal Loss and All Those Confusing Names&lt;/i&gt;. May 2018, https://gombru.github.io/2018/05/23/cross_entropy_loss/.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Gatys2015&quot;&gt;Gatys, Leon, et al. “A Neural Algorithm of Artistic Style.” &lt;i&gt;Journal of Vision&lt;/i&gt;, 2015, doi:10.1167/16.12.326.&lt;/span&gt;&lt;/li&gt;&lt;/ol&gt;
&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;En realidad el área de Wernicke y el área de Broca están conectadas por el fascículo arqueado &lt;a class=&quot;citation&quot; href=&quot;#Neil2012&quot;&gt;(Neil)&lt;/a&gt;. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Solo se muestra la magnitud en este espectrograma. Para calcularlo hay que tomar la transformada de Fourier de la señal, y eso lleva aparejada una fase que no se menciona aquí pero que es fundamental para hacer la transformada inversa después. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Esto aplica en modelos supervisados, en los que conocemos el valor que debería predecir el modelo para cada valor de entrada. Existen otros tipos de algoritmos (no supervisados, por ejemplo), en los que esto no se cumple y la función de &lt;em&gt;loss&lt;/em&gt; adquiere otras formas. &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name></name></author><category term="IA" /><category term="IA" /><category term="imagen" /><category term="lenguaje" /><summary type="html">La pregunta que encabeza este post es recurrente en todos los círculos de la Inteligencia Artificial. Aquellos que trabajan en NLP (procesamiento de lenguaje natural por sus siglas en inglés) buscan concebir sistemas que sean capaces de entender -procesar- lenguaje producido por seres humanos y tomar acciones acordes. GPT-3 es un ejemplo de esto, el cometido de este modelo tan solo es predecir (aunque muy acertadamente) la palabra más probable que seguirá una secuencia de texto (Brown et al.). Sin embargo, por cómo ha sido entrenado, en ningún caso se puede defender que el modelo posea una comprensión profunda de las palabras que está produciendo ni que tenga un objetivo en particular al expresarlas. Si se me permite mi opinión, creo que el salto que nos queda por delante hasta cubrir este último punto es todavía enorme y no vendrá en los próximos años. Al fin y al cabo, todavía no entendemos cómo funciona la mente humana. Así que la respuesta a la pregunta del encabezado será… “depende”, depende de a lo que nos refiramos con la palabra idioma.</summary></entry></feed>