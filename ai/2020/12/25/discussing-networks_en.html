<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Can an AI make up a language? | DidYouKnow</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Can an AI make up a language?" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="The question in the header is recurring in all AI circles. Those who work on NLP (Natural Language Processing) strive to develop systems capable of processing language produced by human beings and take according actions. GPT-3 is an example of this, the goal of this model is just to predict (although very accurately) the most likely word following a text sequence (Brown et al.). However, due to its training process, one cannot defend that the model has a deep comprehension of the words it is producing nor that it has an objective in mind by expressing them. I believe the leap we as human beings have ahead to cover this last hole is still huge and it will not come in the following years. After all, we still do not understand how human mind works. So the answer to the question in the header will be “it depends”. It depends on what we are referring to when we say “language”." />
<meta property="og:description" content="The question in the header is recurring in all AI circles. Those who work on NLP (Natural Language Processing) strive to develop systems capable of processing language produced by human beings and take according actions. GPT-3 is an example of this, the goal of this model is just to predict (although very accurately) the most likely word following a text sequence (Brown et al.). However, due to its training process, one cannot defend that the model has a deep comprehension of the words it is producing nor that it has an objective in mind by expressing them. I believe the leap we as human beings have ahead to cover this last hole is still huge and it will not come in the following years. After all, we still do not understand how human mind works. So the answer to the question in the header will be “it depends”. It depends on what we are referring to when we say “language”." />
<link rel="canonical" href="https://www.javiergamazo.com//blog/ai/2020/12/25/discussing-networks_en.html" />
<meta property="og:url" content="https://www.javiergamazo.com//blog/ai/2020/12/25/discussing-networks_en.html" />
<meta property="og:site_name" content="DidYouKnow" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-12-25T09:00:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Can an AI make up a language?" />
<script type="application/ld+json">
{"mainEntityOfPage":{"@type":"WebPage","@id":"https://www.javiergamazo.com//blog/ai/2020/12/25/discussing-networks_en.html"},"description":"The question in the header is recurring in all AI circles. Those who work on NLP (Natural Language Processing) strive to develop systems capable of processing language produced by human beings and take according actions. GPT-3 is an example of this, the goal of this model is just to predict (although very accurately) the most likely word following a text sequence (Brown et al.). However, due to its training process, one cannot defend that the model has a deep comprehension of the words it is producing nor that it has an objective in mind by expressing them. I believe the leap we as human beings have ahead to cover this last hole is still huge and it will not come in the following years. After all, we still do not understand how human mind works. So the answer to the question in the header will be “it depends”. It depends on what we are referring to when we say “language”.","@type":"BlogPosting","url":"https://www.javiergamazo.com//blog/ai/2020/12/25/discussing-networks_en.html","headline":"Can an AI make up a language?","dateModified":"2020-12-25T09:00:00+00:00","datePublished":"2020-12-25T09:00:00+00:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blog/assets/main.css"><link type="application/atom+xml" rel="alternate" href="https://www.javiergamazo.com//blog/feed.xml" title="DidYouKnow" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/blog/">DidYouKnow</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Can an AI make up a language?</h1>
    <p class="post-meta"><time class="dt-published" datetime="2020-12-25T09:00:00+00:00" itemprop="datePublished">
        Dec 25, 2020
      </time></p>
      <script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_CHTML"></script>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>The question in the header is recurring in all AI circles. Those who work on NLP (Natural Language Processing) strive to develop systems capable of processing language produced by human beings and take according actions. GPT-3 is an example of this, the goal of this model is just to predict (although very accurately) the most likely word following a text sequence <a class="citation" href="#Brown2020">(Brown et al.)</a>. However, due to its training process, one cannot defend that the model has a deep comprehension of the words it is producing nor that it has an objective in mind by expressing them. I believe the leap we as human beings have ahead to cover this last hole is still huge and it will not come in the following years. After all, we still do not understand how human mind works. So the answer to the question in the header will be “it depends”. It depends on what we are referring to when we say “language”.</p>

<p>It is therefore relevant to know what we mean by “language” when we talk about it in this post. We will be referring to a common language shared by several systems through which they understand each other and transmit ideas. However, these ideas will come predefined by an agent external to the AI. A basic example could be to transmit a random number from 0 to 9 from system A to B in a common language, that B would process it, calculate its double, return it to A and A would receive the correct answer (the double of the first number). In this post we’ll try to lay the foundation for a project that meets all those requirements and one more condition: that the language shared by both systems is similar to that of the Star Wars R2-D2 Droid.</p>

<h3 id="it-takes-two-to-tango">It takes two to tango</h3>

<p>Indeed, for communication to exist there must be at least two systems: a transmitter of information and a receiver. Following the Communication Theory of Claude E. Shannon <a class="citation" href="#Shannon1948">(Shannon)</a>, it is also necessary a source of information, a channel, a message, a destination and a noise source. While the precise definition of these components is left to the reader’s discretion, it is of paramount importance for this project to distinguish between <span style="color:blue">information source</span> and <span style="color:red">transmitter</span>, and <span style="color:blue">destination</span> and <span style="color: red">receiver</span>: when we talk about transmitter we refer to the technical system in charge of encoding the information source to a set of signals suitable for the channel (and vice versa for the receiver, which decodes it so that the receiver can use the information). Applied to verbal communication between two people A and B, the source of information will be A’s idea, whose brain will be in charge of coding it in the form of words - message - that will be sent through the air - channel - until it reaches B. B’s brain will decode the words and generate an idea, which in the best case will be the same as the idea of A’s brain.</p>

<p><img src="/blog/assets/1.discussing/images/information_theory_diagram_en.png" alt="Information theory" class="image-centered" width="700px" />
<em>Figure 1: Shannon’s Information Theory diagram. Adapted from <a class="citation" href="#Shannon1948">(Shannon)</a>.</em></p>

<p>Let us now transform this theory into the field of AI. As we know, neural networks are tremendously effective in generating representations from data, so we could face two of them (subjects A and B) and make them share messages. A would encode the message and B would… decode it? And what would happen if B wanted to transmit an idea to A? To find the solution to this question we can look at the most efficient system we know when it comes to producing and processing language: the brain. This organ takes care of both functions simultaneously (or can’t you listen to music and talk at the same time?), and that is because the information flow passes through separate areas<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote">1</a></sup>. These two zones are Wernicke’s area and Broca’s area, which are in charge of understanding and producing language respectively <a class="citation" href="#Neil2012">(Neil)</a>. Following this example, it seems logical to make our subjects be formed by two different and disconnected networks. One of these networks, the <em>encoder</em> will be in charge of producing the language while the <em>decoder</em> will try to understand it. In case A wants to transmit an idea to B, A’s <em>encoder</em> will be responsible for transforming this idea into a common language and B’s <em>decoder</em> will interpret it.</p>

<p><img src="/blog/assets/1.discussing/images/estructura.png" alt="Structure" width="500px" class="image-centered" />
<em>Figure 2: Structure of a system with two subjects.</em></p>

<h3 id="talking-as-clear-as-r2-d2">Talking as clear as R2-D2</h3>

<p>I will first describe R2-D2’s way of communicating for those who do not know it. It is a series of beeps, whistles and other sounds agglutinated to form something that resembles phrases <a class="citation" href="#Bray2015">(Bray et al.)</a>. On YouTube there are <a href="https://www.youtube.com/watch?v=2-BKjnAgNgY" target="_blank">several</a> videos showing these sounds. We will try to make the neural networks imitate this language when speaking, forcing the representation of the idea by the encoder to also be a series of beeps. However, forcing this type of representation is not trivial for two reasons: first, language must be created spontaneously through conversation between the networks, there must be no human interaction in this process. Secondly, we must define what we mean by “representation” in this context since language is very varied. We will address the second question first, which will lead us inexorably to the solution of the first._</p>

<p>A sound can be represented in three different ways depending on what we want to know about it:</p>
<ul>
  <li><strong>The temporal representation</strong> shows the intensity of the sound as a function of time.</li>
  <li><strong>Representation in frequencies:</strong> explained very briefly, all sound, because it is a wave as a function of time, can be broken down into more basic waves with different frequencies. The method by which we go from a wave in the time domain to a wave in the frequency domain is called <em>Fourier transform</em>. The representation in frequencies contains information on how important (amplitude) each frequency is in giving rise to the underlying sound.</li>
  <li><strong>Representation in the form of a spectrogram:</strong> the two previous representations have two dimensions, i.e. amplitude as a function of time or amplitude of each frequency. However, we can join both to produce a three-dimensional representation: time, amplitude and frequency. A spectrogram shows the evolution of frequency and intensity in time. Typically, intensity is defined by color, while time takes the abscissa axis and frequency the ordinate axis. Unlike the previous cases, a spectrogram can be saved as an image since the relevant information is coded both in the color of the pixels and in their position. The following figure shows the spectrogram of a voice fragment of R2-D2<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote">2</a></sup>.</li>
</ul>

<p><img src="/blog/assets/1.discussing/images/espectrograma.png" alt="Spectrogram" class="image-centered" />
<em>Figure 3: Spectrogram of a sound produced by R2-D2. It has been calculated by taking the Fourier transform of all the audio of the video in <a href="https://www.youtube.com/watch?v=2-BKjnAgNgY">https://www.youtube.com/watch?v=2-BKjnAgNgY</a> and dividing the result in 256 pixels images.</em></p>

<p>It has been shown that neural networks operate well with tabulated data and especially well with images <a class="citation" href="#Krizhevsky2012">(Krizhevsky et al.)</a><a class="citation" href="#He2016">(He et al.)</a>, so it seems logical to use the spectrogram for this task. In general, convolutional neural networks are used to work with images because they are capable of capture relations in their pixels, usually by identifying borders in different directions or other coarser features like textures. Explaining in detail how this type of networks work would be useful for another article, so it is left to the interested reader to become familiar with this type of architectures, since there are many resources available (<a class="citation" href="#Cs231n">(Stanford)</a>, for example). I will only say here that with a network of convolutional layers we can obtain a multi-dimensional representation of the input data (an image). That is, we can get the translation of any number in an image, and this translation can be learned for a concrete task.</p>

<h3 id="a-spoonful-of-architecture">A spoonful of architecture</h3>

<p>Once all the technical aspects of the problem have been addressed, we will focus on defining the system architecture and the training process. Determining network architecture is usually more of an art than a science, so I will not go into detail. First, we will code the idea (the number) in a One-Hot way, which means that to transmit a digit from \( N \) possible ones we will use a vector of  \( N \) components with zero in all of them and one in the relevant component. For example, the vector for the number 1 in the interval \( [0, 4] \) will be \([0, 1, 0, 0, 0]\). To use this data with convolutional networks we will have to transform it to three dimensions ( (C\times H\times W ) \), increasing first the amount of components with at least one fully connected layer and resizing the result. Later, we will add convolutional layers so that the result is a map of the dimensions of the spectrogram we are looking for. As the task we are trying to solve is symmetric (the encoder and the decoder must perform opposite tasks), we will opt for symmetric architectures for these two networks. If we have said that the encoder will start with a fully connected layer followed by convolutional layers, the decoder will start with convolutional layers and finally another fully connected one. In this way, the diagram above takes on the following structure:</p>

<p><img src="/blog/assets/1.discussing/images/estructura_espectrograma_en.png" alt="Structure with spectrogram" width="650px" class="image-centered" />
<em>Figure 3: Structure of a system with two subjects where number 1 is sent. By comparison with Shannon’s information theory, the central spectrogram shows the message. Only one part of the system is shown for the sake of simplicity.</em></p>

<p>To make a neural network system behave in a certain way, one must specify a quantity to minimize. This amount is called <em>loss</em>, and in many cases it is a function that depends on the network output and the original data<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote">3</a></sup>. Applied to our problem, we can immediately define a quantity that the system should minimize: we want B to interpret the same idea that A is trying to transmit. As there is only one correct number each time, the problem is multiclass (predict a number and only one of ten possible) and we will minimize the <em>Cross-Entropy loss</em>, defined as:
\[ L_{CE} = - \sum_i^C t_i \log(s_i),\]
where \( C \) are the possible classes and \( t_i \) and \( s_i \) represent the real and predicted labels, respectively<a class="citation" href="#Gomez2018">(Gómez)</a>.</p>

<p>By minimizing the Cross-Entropy loss we get the two networks to agree on the number they are transmitting. It is important to point out that only by minimizing this function the networks would create an internal language at the output of the encoder, but this would very likely be random. As we want them to use R2-D2’s voice, we will have to add a function of loss more and we will do it… with style.</p>

<h3 id="speaking-with-style">Speaking with style</h3>

<p>As explained above, language must be created spontaneously and without human interaction, and for this purpose representation in the form of a spectrogram can help us. As it is evident, we cannot force the output of the encider to be equal (pixel by pixel) to a concrete spectrogram or to a group of these since we would be violating the second assumption of the problem (there would exist in that case strong human interaction). We will need then a less intrusive loss function, that does not look for differences by pixel but something more general. Fortunately, in 2015 a type of loss was introduced that is suitable for this use case: the style loss <a class="citation" href="#Gatys2015">(Gatys et al.)</a>. Intuitively, this loss is calculated by passing the images through a pre-trained neural network (VGG) and comparing the intermediate representations of the target image and the generated image. The authors apply this idea to transfer the style of a picture to a photo, keeping the underlying idea of the original photo intact. For anyone interested, mathematically it looks like this with \( l \) intermediate representations:</p>

<p>\[ L_{style} = \sum_l w^lL_{style}^l,   \]
\[ L_{style}^l = \dfrac{1}{M^l} \sum_{ij} (G_{ij}^l(s) - G_{ij}^l(g))^2,   \]
where \( G(s) \) and \( G(g) \) refer to the Gram matrix of the style image and the generated image. For further information, I think <a href="https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-neural-style-transfer-ef88e46697ee">this article</a> can be interesting.</p>

<p>We can use the style loss to induce the output of the encoder to be similar to the reference spectrograms. This will presumably keep the sounds random but the result will remind us of the movie droid. However, this function cannot be applied to both encoder and decoder. Since only the encoder is involved in the language creation process, it is the only one that should receive feedback in this regard. However, the idea must flow through both networks, so the Cross-Entropy loss must spread throughout the system. Thus, the functions in each case result:</p>

<p>\[ L_{decoder} = L_{CE} = - \sum_i^C t_i \log(s_i),\]
\[ L_{encoder} = L_{CE} + \lambda L_{style} = - \sum_i^C t_i \log(s_i) + \lambda \sum_l w^l \dfrac{1}{M^l} \sum_{ij} (G_{ij}^l(s) - G_{ij}^l(g))^2\]</p>

<p>And the whole system is:</p>

<p><img src="/blog/assets/1.discussing/images/estructura_loss_en.png" alt="Structure with loss" width="650px" class="image-centered" />
<em>Figure 4: Propagation and computation of the loss function throughout the system. Black: forward propagation. Red: backpropagation of the loss function.</em></p>

<p>One has to notice that the simplicity of the message we are transmitting can be a problem in order to train the encoder to generate an adequate representation, as it may cause the loss function to be unbalanced. Later on, we will make transmission harder with the addition of noise to the intermediate representation, thus making the input of the decoder different from the output of the encoder, perturbing the system and avoiding \(L_{decoder} = L_{CE} = 0 \) after a couple of training steps.</p>

<p>All that remains is choosing a training strategy. As described above, each subject is formed by two networks with different tasks and each communication faces different parts of each subject. Therefore, with \( N \) subjects we will have  \( 2N \) networks and  \( N^2 \) ways to face them (counting on the fact that we want a subject to be able to understand himself). To train all the subjects at the same time and avoid that some nets acquire more level than others we will have to follow a staggered strategy, alternating the training steps between all the combinations. Thus, both the time and the complexity of the training grow with \( \mathcal{O}(N^2) \). This strategy will also prevent the system from learning \( N^2 \) different languages, as all the networks will be learning in parallel.</p>

<p>Thus concludes the first part of this article on how to devise a system of neural networks from which a common language can emerge and how to make this language have the form we want. The second part will explore the results and other training strategies based on these. In addition, a noise source will be included to make the training more robust.</p>

<hr />

<hr />

<ol class="bibliography"><li><span id="Brown2020">Brown, Tom B., et al. <i>Language Models Are Few-Shot Learners</i>. May 2020, https://arxiv.org/abs/2005.14165.</span></li>
<li><span id="Shannon1948">Shannon, C. E. “A Mathematical Theory of Communication.” <i>Bell System Technical Journal</i>, 1948, doi:10.1002/j.1538-7305.1948.tb01338.x.</span></li>
<li><span id="Neil2012">Neil, Carlson. “Physiology of Behavior.” <i>IEEE Transactions on Information Theory</i>, 2012.</span></li>
<li><span id="Bray2015">Bray, Adam, et al. <i>Star Wars: Absolutely Everything You Need to Know</i>. DK Children, 2015, p. 200.</span></li>
<li><span id="Krizhevsky2012">Krizhevsky, Alex, et al. “ImageNet Classification with Deep Convolutional Neural Networks.” <i>Advances in Neural Information Processing Systems</i>, 2012, doi:10.1061/(ASCE)GT.1943-5606.0001284.</span></li>
<li><span id="He2016">He, Kaiming, et al. “Deep Residual Learning for Image Recognition.” <i>Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</i>, 2016, doi:10.1109/CVPR.2016.90.</span></li>
<li><span id="Cs231n">Stanford, Cs231n. <i>CS231n Convolutional Neural Networks for Visual Recognition</i>. https://cs231n.github.io/convolutional-networks/. Accessed December 21, 2020.</span></li>
<li><span id="Gomez2018">Gómez, Raúl. <i>Understanding Categorical Cross-Entropy Loss, Binary Cross-Entropy Loss, Softmax Loss, Logistic Loss, Focal Loss and All Those Confusing Names</i>. May 2018, https://gombru.github.io/2018/05/23/cross_entropy_loss/.</span></li>
<li><span id="Gatys2015">Gatys, Leon, et al. “A Neural Algorithm of Artistic Style.” <i>Journal of Vision</i>, 2015, doi:10.1167/16.12.326.</span></li></ol>
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>Actually the Wernicke area and the Broca area are connected by the arcuate fasciculus <a class="citation" href="#Neil2012">(Neil)</a>. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>Only the magnitude is shown in this spectrogram. To calculate it, the Fourier transform of the signal has to be computed, and that carries a phase that is not mentioned here but that is fundamental to make the inverse transform later. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p>This applies for supervised models, where we know the value the model should predict for each input value. There are other types of algorithms (unsupervised, for example), in which this is not fulfilled and the loss function takes other forms. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

  </div><a class="u-url" href="/blog/ai/2020/12/25/discussing-networks_en.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">DidYouKnow</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">DidYouKnow</li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/javirk"><svg class="svg-icon"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg> <span class="username">javirk</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>The place where I talk about anything</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
