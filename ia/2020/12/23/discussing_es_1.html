<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.7.1 -->
<title>¿Puede una IA inventar un idioma? | DidYouKnow</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="¿Puede una IA inventar un idioma?" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="La pregunta que encabeza este post es recurrente en todos los círculos de la Inteligencia Artificial. Aquellos que trabajan en NLP (procesamiento de lenguaje natural por sus siglas en inglés) buscan concebir sistemas que sean capaces de entender - procesar - lenguaje producido por seres humanos y tomar acciones acordes. GPT-3 es un ejemplo de esto, el cometido de este modelo tan solo es predecir (aunque muy acertadamente) la palabra más probable que seguirá una secuencia de texto (Brown et al.). Sin embargo, por cómo ha sido entrenado, en ningún caso se puede defender que el modelo posea una comprensión profunda de las palabras que está produciendo ni que tenga un objetivo en particular al expresarlas. Si se me permite mi opinión, creo que el salto que nos queda por delante hasta cubrir este último punto es todavía enorme y no vendrá en los próximos años. Al fin y al cabo, todavía no entendemos cómo funciona la mente humana." />
<meta property="og:description" content="La pregunta que encabeza este post es recurrente en todos los círculos de la Inteligencia Artificial. Aquellos que trabajan en NLP (procesamiento de lenguaje natural por sus siglas en inglés) buscan concebir sistemas que sean capaces de entender - procesar - lenguaje producido por seres humanos y tomar acciones acordes. GPT-3 es un ejemplo de esto, el cometido de este modelo tan solo es predecir (aunque muy acertadamente) la palabra más probable que seguirá una secuencia de texto (Brown et al.). Sin embargo, por cómo ha sido entrenado, en ningún caso se puede defender que el modelo posea una comprensión profunda de las palabras que está produciendo ni que tenga un objetivo en particular al expresarlas. Si se me permite mi opinión, creo que el salto que nos queda por delante hasta cubrir este último punto es todavía enorme y no vendrá en los próximos años. Al fin y al cabo, todavía no entendemos cómo funciona la mente humana." />
<link rel="canonical" href="https://www.javiergamazo.com//blog/ia/2020/12/23/discussing_es_1.html" />
<meta property="og:url" content="https://www.javiergamazo.com//blog/ia/2020/12/23/discussing_es_1.html" />
<meta property="og:site_name" content="DidYouKnow" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-12-23T09:00:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="¿Puede una IA inventar un idioma?" />
<script type="application/ld+json">
{"description":"La pregunta que encabeza este post es recurrente en todos los círculos de la Inteligencia Artificial. Aquellos que trabajan en NLP (procesamiento de lenguaje natural por sus siglas en inglés) buscan concebir sistemas que sean capaces de entender - procesar - lenguaje producido por seres humanos y tomar acciones acordes. GPT-3 es un ejemplo de esto, el cometido de este modelo tan solo es predecir (aunque muy acertadamente) la palabra más probable que seguirá una secuencia de texto (Brown et al.). Sin embargo, por cómo ha sido entrenado, en ningún caso se puede defender que el modelo posea una comprensión profunda de las palabras que está produciendo ni que tenga un objetivo en particular al expresarlas. Si se me permite mi opinión, creo que el salto que nos queda por delante hasta cubrir este último punto es todavía enorme y no vendrá en los próximos años. Al fin y al cabo, todavía no entendemos cómo funciona la mente humana.","@type":"BlogPosting","url":"https://www.javiergamazo.com//blog/ia/2020/12/23/discussing_es_1.html","headline":"¿Puede una IA inventar un idioma?","dateModified":"2020-12-23T09:00:00+00:00","datePublished":"2020-12-23T09:00:00+00:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://www.javiergamazo.com//blog/ia/2020/12/23/discussing_es_1.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blog/assets/main.css"><link type="application/atom+xml" rel="alternate" href="https://www.javiergamazo.com//blog/feed.xml" title="DidYouKnow" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/blog/">DidYouKnow</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">¿Puede una IA inventar un idioma?</h1>
    <p class="post-meta"><time class="dt-published" datetime="2020-12-23T09:00:00+00:00" itemprop="datePublished">
        Dec 23, 2020
      </time></p>
      <script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_CHTML"></script>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>La pregunta que encabeza este post es recurrente en todos los círculos de la Inteligencia Artificial. Aquellos que trabajan en NLP (procesamiento de lenguaje natural por sus siglas en inglés) buscan concebir sistemas que sean capaces de entender - procesar - lenguaje producido por seres humanos y tomar acciones acordes. GPT-3 es un ejemplo de esto, el cometido de este modelo tan solo es predecir (aunque muy acertadamente) la palabra más probable que seguirá una secuencia de texto <a class="citation" href="#Brown2020">(Brown et al.)</a>. Sin embargo, por cómo ha sido entrenado, en ningún caso se puede defender que el modelo posea una comprensión profunda de las palabras que está produciendo ni que tenga un objetivo en particular al expresarlas. Si se me permite mi opinión, creo que el salto que nos queda por delante hasta cubrir este último punto es todavía enorme y no vendrá en los próximos años. Al fin y al cabo, todavía no entendemos cómo funciona la mente humana.</p>

<p>Es por tanto relevante saber a qué nos referimos con “idioma” o “lenguaje” cuando hablemos de ello en este post. Nos estaremos refiriendo a un lenguaje común que compartan varios sistemas a través del cual puedan llegar a entenderse y transmitir ideas. No obstante, estas ideas vendrán predefinidas por un agente externo a la IA. Un ejemplo básico podría ser transmitir un número aleatorio de 0 a 9 del sistema A al B en un lenguaje común, que el B lo procesara, calculara su doble, lo devolviera a A y éste diera con la respuesta correcta. En este post intentaremos sentar las bases de un proyecto que cumpla todos esos requisitos y una condición más: que el idioma que compartan ambos sistemas sea similar al del droide R2-D2 de Star Wars.</p>

<h3 id="dos-no-hablan-si-uno-no-quiere">Dos no hablan si uno no quiere</h3>

<p>Efectivamente, para que exista comunicación debe haber al menos dos sistemas: un emisor de información (transmisor) y otro receptor de la misma. Siguiendo la Teoría de la Comunicación de Claude E. Shannon <a class="citation" href="#Shannon1948">(Shannon)</a>, es necesario además una fuente de información, un canal, un mensaje, un destinatario y un elemento de ruido. Si bien la definición precisa de estos componentes se deja a discreción del lector, es de capital importancia para este proyecto distinguir entre <span style="color:blue">fuente de información</span> y <span style="color:red">transmisor</span>, y <span style="color:blue">destinatario</span> y <span style="color:red">receptor</span>: cuando hablamos de transmisor nos referimos al sistema técnico encargado de <em>codificar</em> la fuente de información a un conjunto de señales aptas para el canal (y viceversa para el receptor, que la <em>decodifica</em> para que el destinatario pueda utilizar la información). Aplicado a la comunicación verbal entre dos personas A y B, la fuente de información será la idea de A, cuyo cerebro se encargará de codificar en forma de palabras - mensaje - que se enviarán por el aire - canal - hasta llegar a B. El cerebro de B decodificará las palabras y generará una idea, que en el mejor de los casos será igual a la idea del cerebro de A.</p>

<p><img src="/blog/assets/1.discussing/images/information_theory_diagram_es.png" alt="Information theory" class="image-centered" width="700px" />
<em>Figura 1: Diagrama de la teoría de la información de Shannon. Adaptado de <a class="citation" href="#Shannon1948">(Shannon)</a>.</em></p>

<p>Transformemos ahora esta teoría al campo de la IA. Como sabemos, las redes neuronales son tremendamente eficaces a la hora de generar representaciones a partir de datos, por lo que podríamos enfrentar dos de ellas (sujetos A y B) y hacerlas compartir mensajes. A codificaría el mensaje y B lo… ¿decodificaría? ¿Y qué pasaría si B quisiera transmitir una idea a A? Para encontrar la solución a esta pregunta podemos fijarnos en el sistema más eficiente que conocemos a la hora de producir y procesar lenguaje: el cerebro. Este órgano se encarga de ambas funciones simultáneamente (¿o es que no puedes escuchar música y hablar a la vez?), y eso se debe a que el flujo de información transcurre por zonas separadas<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote">1</a></sup>. Estas dos zonas son el área de Wernicke y el área de Broca, que se encargan de la comprensión y de la producción del lenguaje respectivamente <a class="citation" href="#Neil2012">(Neil)</a>. Siguiendo este ejemplo, parece lógico hacer que nuestros sujetos estén formados por dos redes diferentes y desconectadas. Una de estas redes, el <em>encoder</em> se encargará de producir el lenguaje mientras que el <em>decoder</em> tratará de comprenderlo. En caso de que A quiera transmitir una idea a B, será el <em>encoder</em> de A el responsable de transformar esta idea a un lenguaje común y el <em>decoder</em> de B lo interpretará.</p>

<p><img src="/blog/assets/1.discussing/images/estructura.png" alt="Estructura" width="500px" class="image-centered" />
<em>Figura 2: Estructura de un sistema con dos individuos.</em></p>

<h3 id="hablemos-tan-claro-como-r2-d2">Hablemos tan claro como R2-D2</h3>

<p>Describiré en primer lugar la forma de comunicarse de R2-D2 para aquellos que la desconozcan. Se trata de una serie de pitidos, silbidos y otros sonidos aglutinados para formar algo que se asemeja a frases <a class="citation" href="#Bray2015">(Bray et al.)</a>. En YouTube hay <a href="https://www.youtube.com/watch?v=2-BKjnAgNgY" target="_blank">infinidad</a> de vídeos mostrando estos sonidos (se abre en otra pestaña). Intentaremos hacer que las redes neuronales imiten este lenguaje a la hora de hablar, forzando que la representación de la idea por parte del <em>encoder</em> sea también una serie de pitidos. Sin embargo, forzar este tipo de representación no es trivial por dos motivos: en primer lugar, el lenguaje debe ser creado espontáneamente a través de la conversación entre las redes, no debe existir ningún tipo de interacción humana en este proceso. En segundo lugar, debemos definir a qué nos referimos con “representación” en este contexto puesto que el lenguaje es muy variado. Abordaremos primero la segunda cuestión, lo que nos llevará inexorablemente hasta la solución de la primera.</p>

<p>Un sonido puede ser representado de tres formas diferentes en función de lo que busquemos conocer acerca del mismo:</p>
<ul>
  <li><strong>Representación temporal:</strong> muestra la intensidad del sonido en función del tiempo.</li>
  <li><strong>Representación en frecuencias:</strong> explicado muy brevemente, todo sonido, por el hecho de ser una onda en función del tiempo, puede ser descompuesto en ondas más básicas con diferentes frecuencias. El método con el que pasamos de una onda en el dominio temporal a una onda en el dominio de frecuencias recibe el nombre de <em>transformada de Fourier</em>. La representación en frecuencias contiene información de lo importante (amplitud) que es cada frecuencia para dar lugar al sonido subyacente.</li>
  <li><strong>Representación en forma de espectrograma:</strong> las dos representaciones anteriores tienen dos dimensiones, i.e. amplitud en función del tiempo o amplitud de cada frecuencia. Sin embargo, podemos unir ambas para dar lugar a una representación tridimensional: tiempo, amplitud y frecuencia. Un espectrograma muestra la evolución de la frecuencia y de la intensidad en el tiempo. Típicamente, la intensidad se define por el color, mientras que el tiempo toma el eje de abscisas y la frecuencia, el de ordenadas. A diferencia de en los casos anteriores, un espectrograma puede ser guardado en forma de imagen ya que la informaciónm relevante está codificada tanto en el color de los píxeles como en su posición. La siguiente figura muestra el espectrograma de un fragmento de la voz de R2-D2<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote">2</a></sup>.</li>
</ul>

<p><img src="/blog/assets/1.discussing/images/espectrograma.png" alt="Espectrograma" class="image-centered" />
<em>Figura 3: Espectrograma de un sonido producido por R2-D2. Se ha calculado tomando la transformada de Fourier de todo el audio del vídeo en <a href="https://www.youtube.com/watch?v=2-BKjnAgNgY">https://www.youtube.com/watch?v=2-BKjnAgNgY</a> y dividiendo el resultado en imágenes de 256 píxeles.</em></p>

<p>Se ha demostrado que las redes neuronales operan bien con datos tabulados y especialmente bien con imágenes <a class="citation" href="#Krizhevsky2012">(Krizhevsky et al.)</a><a class="citation" href="#He2016">(He et al.)</a>, así que parece lógico utilizar el espectograma para esta tarea. En general, para trabajar con imágenes se utilizan redes neuronales convolucionales. Explicar en detalle cómo funcionan este tipo de redes daría para otro artículo, por lo que se deja al lector interesado el familiarizarse con este tipo de arquitecturas, ya que existen multitud de recursos disponibles (<a class="citation" href="#DotCSV2020">(Dot CSV)</a>, <a class="citation" href="#Cs231n">(Stanford)</a> [en inglés], por ejemplo). Tan solo diré aquí que con una red de capas convolucionales podemos obtener una representación en varias dimensiones de los datos de entrada (una imagen). Es decir, podemos conseguir la traducción de un número cualquiera en una imagen, y esta traducción puede ser aprendida para una tarea concreta.</p>

<h3 id="un-poco-de-arquitectura">Un poco de arquitectura</h3>

<p>Una vez abordados todos los aspectos técnicos del problema nos centraremos en definir la arquitectura del sistema y la forma de entrenamiento. Determinar la arquitectura de las redes generalmente tiene más de arte que de ciencia, por lo que no me explayaré en los detalles. En primer lugar, codificaremos la idea (el número) de forma <em>One-Hot</em>, lo que significa que para transmitir un dígito de entre \( N \) posibles utilizaremos un vector de \( N \) componentes con cero en todas ellas y un uno en la componente relevante. Por ejemplo, el vector para el número 1 en el intervalo \( [0, 4] \) será \([0, 1, 0, 0, 0]\). Para utilizar estos datos con redes convolucionales deberemos transformarlos a tres dimensiones \( (C\times H\times W ) \), aumentando en primer lugar la cantidad de componentes con al menos una capa <em>fully connected</em> y redimensionando el resultado. Posteriormente, añadiremos capas convolucionales de forma que el resultado sea un mapa de las dimensiones del espectrograma que buscamos. Como la tarea que buscamos resolver es simétrica (el <em>encoder</em> y el <em>decoder</em> deben realizar tareas opuestas), optaremos por arquitecturas simétricas para estas dos redes. Si hemos dicho que el <em>encoder</em> empezará con una capa <em>fully connected</em> seguida de capas convolucionales, el <em>decoder</em> empezará por capas convolucionales y por último otra <em>fully connected</em>. De esta forma, el diagrama anterior adquiere la siguiente estructura:</p>

<p><img src="/blog/assets/1.discussing/images/estructura_espectrograma_es.png" alt="Estructura con espectrograma" width="650px" class="image-centered" />
<em>Figura 3: Estructura de un sistema con dos individuos en el que se envía el número 1. Solo se muestra una parte del sistema (conexión de A con B) por simplicidad.</em></p>

<p>Para hacer que un sistema de redes neuronales se comporte de una forma determinada se debe especificar una cantidad a minimizar. A esta cantidad se le llama <em>loss</em>, y en muchos casos es una función que depende de la salida de la red y de los datos originales <sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote">3</a></sup>. Aplicado a nuestro problema, podemos definir inmdieatamente una cantidad que el sistema debe minimizar: queremos que B interprete la misma idea que está intentando transmitir A. Como solo hay un número correcto cada vez, el problema es multiclase (predecir un número y solo uno de entre diez posibles) y minimizaremos la <em>Cross-Entropy loss</em>, definida como:
\[ L_{CE} = - \sum_i^C t_i \log(s_i),\]
donde \( C \) son las clases posibles y \( t_i \) y \( s_i \) representan la etiqueta real y la predicha por la red respectivamente <a class="citation" href="#Gomez2018">(Gómez)</a>.</p>

<p>Minimizando la <em>Cross-Entropy loss</em> conseguimos que las dos redes se pongan de acuerdo en el número que están transmitiendo. Es importante puntualizar que tan solo minimizando esta cantidad las redes ya crearían un lenguaje interno a la salida del <em>encoder</em>, pero éste sería probablemente aleatorio. Como queremos que utilicen la voz de R2-D2, tendremos que añadir una función de <em>loss</em> más y lo haremos… con estilo.</p>

<h3 id="hablando-con-estilo">Hablando con estilo</h3>

<p>Como se ha explicado más arriba, el lenguaje debe ser creado espontáneamente y sin interacción humana, y para ello la representación en forma de espectrograma nos puede ayudar. Como es evidente, no podemos forzar que la salida del <em>encoder</em> sea igual (píxel a píxel) a un espectrograma concreto ni a un grupo de estos puesto que estaríamos violando el segundo supuesto del problema (existiría en ese caso fuerte interacción humana). Necesitaremos entonces una función de <em>loss</em> menos intrusiva, que no busque diferencias por píxel sino algo más general. Por suerte, en 2015 se introdujo un tipo de <em>loss</em> idóneo para este caso de uso: la <em>loss</em> de estilo <a class="citation" href="#Gatys2015">(Gatys et al.)</a>. Intuitivamente, esta <em>loss</em> se calcula haciendo pasar las imágenes por una red neuronal preentrenada (VGG) y comparando las representaciones intermedias de la imagen objetivo y la imagen generada. Los autores aplican esta idea para transferir el estilo de un cuadro a una foto, manteniendo intacta la idea subyacente de la foto original. Para quien esté interesado, matemáticamente tiene este aspecto con \( l \) representaciones intermedias:</p>

<p>\[ L_{estilo} = \sum_l w^lL_{estilo}^l,   \]
\[ L_{estilo}^l = \dfrac{1}{M^l} \sum_{ij} (G_{ij}^l(s) - G_{ij}^l(g))^2,   \]
donde \( G(s) \) y \( G(g) \) se refieren a la matriz de Gram de la imagen estilo y la imagen generada respectivamente. Para más información, creo que <a href="https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-neural-style-transfer-ef88e46697ee">este artículo</a> puede ser interesante.</p>

<p>Podemos utilizar la <em>loss</em> de estilo para inducir que la salida del <em>encoder</em> sea similar a los espectrogramas de referencia. Con ello presumiblemente conseguiremos que los sonidos se mantengan aleatorios pero nos recuerden a los del droide de las películas. No obstante, esta función no puede ser aplicada a <em>encoder</em> y <em>decoder</em> por igual. Puesto que solo el <em>encoder</em> participa en el proceso de creación del lenguaje, es el único que debe recibir <em>feedback</em> en ese aspecto. Sin embargo, la idea debe fluir a través de ambas redes, por lo que la <em>Cross-Entropy loss</em> se debe propagar por todo el sistema. Así, las funciones en cada caso resultan:
\[ L_{decoder} = L_{CE} = - \sum_i^C t_i \log(s_i),\]
\[ L_{encoder} = L_{CE} + \lambda L_{estilo} = - \sum_i^C t_i \log(s_i) + \lambda \sum_l w^l \dfrac{1}{M^l} \sum_{ij} (G_{ij}^l(s) - G_{ij}^l(g))^2\]</p>

<p>Y el sistema total queda como en la Figura 4:</p>

<p><img src="/blog/assets/1.discussing/images/estructura_loss_es.png" alt="Estructura con loss" width="650px" class="image-centered" />
<em>Figura 4: Propagación y cálculo de las funciones de _loss_ a través del sistema. En negro, propagación hacia delante. En rojo, propagación de las funciones de loss hacia atrás.</em></p>

<p>Solo restaría elegir una estrategia de entrenamiento. Como se ha descrito más arriba, cada sujeto está formado por dos redes con cometidos diferentes y cada comunicación enfrenta partes distintas de cada sujeto. Por lo tanto, con \( N \) sujetos tendremos \( 2N \) redes y \( N^2 \) formas de enfrentarlas (contando con que queremos que un sujeto pueda entenderse a sí mismo). Para entrenar a todos los sujetos a la vez y evitar que algunas redes adquieran más nivel que otras tendremos que seguir una estrategia escalonada, alternando los pasos de entrenamiento entre todas las combinaciones. Así, tanto el tiempo como la complejidad del entrenamiento crecen con \( \mathcal{O}(N^2) \).</p>

<p>Hasta aquí la primera parte de este artículo sobre cómo idear un sistema de redes neuronales del que pueda emerger un lenguaje común y cómo hacer que este lenguaje tenga la forma que queramos. En la segunda parte se explorarán los resultados y otras estrategias de entrenamiento en función de estos. Además, se incluirá una fuente de ruido para hacer el entrenamiento más robusto.</p>

<hr />

<hr />

<ol class="bibliography"><li><span id="Brown2020">Brown, Tom B., et al. <i>Language Models Are Few-Shot Learners</i>. May 2020, https://arxiv.org/abs/2005.14165.</span></li>
<li><span id="Shannon1948">Shannon, C. E. “A Mathematical Theory of Communication.” <i>Bell System Technical Journal</i>, 1948, doi:10.1002/j.1538-7305.1948.tb01338.x.</span></li>
<li><span id="Neil2012">Neil, Carlson. “Physiology of Behavior.” <i>IEEE Transactions on Information Theory</i>, 2012.</span></li>
<li><span id="Bray2015">Bray, Adam, et al. <i>Star Wars: Absolutely Everything You Need to Know</i>. DK Children, 2015, p. 200.</span></li>
<li><span id="Krizhevsky2012">Krizhevsky, Alex, et al. “ImageNet Classification with Deep Convolutional Neural Networks.” <i>Advances in Neural Information Processing Systems</i>, 2012, doi:10.1061/(ASCE)GT.1943-5606.0001284.</span></li>
<li><span id="He2016">He, Kaiming, et al. “Deep Residual Learning for Image Recognition.” <i>Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</i>, 2016, doi:10.1109/CVPR.2016.90.</span></li>
<li><span id="DotCSV2020">Dot CSV. <i>¿Qué Es Una Red Neuronal Convolucional? Los OJOS De La Inteligencia Artificial - YouTube</i>. 2020, https://www.youtube.com/watch?v=V8j1oENVz00.</span></li>
<li><span id="Cs231n">Stanford, Cs231n. <i>CS231n Convolutional Neural Networks for Visual Recognition</i>. https://cs231n.github.io/convolutional-networks/. Accessed December 21, 2020.</span></li>
<li><span id="Gomez2018">Gómez, Raúl. <i>Understanding Categorical Cross-Entropy Loss, Binary Cross-Entropy Loss, Softmax Loss, Logistic Loss, Focal Loss and All Those Confusing Names</i>. May 2018, https://gombru.github.io/2018/05/23/cross_entropy_loss/.</span></li>
<li><span id="Gatys2015">Gatys, Leon, et al. “A Neural Algorithm of Artistic Style.” <i>Journal of Vision</i>, 2015, doi:10.1167/16.12.326.</span></li></ol>
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>En realidad el área de Wernicke y el área de Broca están conectadas por el fascículo arqueado <a class="citation" href="#Neil2012">(Neil)</a>. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>Solo se muestra la magnitud en este espectrograma. Para calcularlo hay que tomar la transformada de Fourier de la señal, y eso lleva aparejada una fase que no se menciona aquí pero que es fundamental para hacer la transformada inversa después. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p>Esto aplica en modelos supervisados, en los que conocemos el valor que debería predecir el modelo para cada valor de entrada. Existen otros tipos de algoritmos (no supervisados, por ejemplo), en los que esto no se cumple y la función de <em>loss</em> adquiere otras formas. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

  </div><a class="u-url" href="/blog/ia/2020/12/23/discussing_es_1.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">DidYouKnow</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">DidYouKnow</li><li><a class="u-email" href="mailto:your-email@example.com">your-email@example.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/javirk"><svg class="svg-icon"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg> <span class="username">javirk</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>The place where I talk about anything</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
